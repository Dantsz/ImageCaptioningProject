{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a2843797",
      "metadata": {
        "id": "a2843797"
      },
      "source": [
        "# Fetch dataset\n",
        "I will be working with COCO Captions, which is a large-scale object detection, segmentation, and captioning dataset. It contains over 330k images, with more than 200k labeled images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cf135fdd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf135fdd",
        "outputId": "18d68585-28a1-4ef5-eaa3-e0110903aa12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting loguru\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Downloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: loguru\n",
            "Successfully installed loguru-0.7.3\n",
            "env: KAGGLEHUB_CACHE=datasets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1mINFO\u001b[0m: Importing dataset to datasets | __main__:<cell line: 0>:13 | 18:40:52 29-03-2025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/nikhil7280/coco-image-caption?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13.7G/13.7G [02:25<00:00, 101MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\u001b[1mINFO\u001b[0m: Path to dataset files: | __main__:<cell line: 0>:15 | 18:46:25 29-03-2025\n",
            "\u001b[1mINFO\u001b[0m: Running on cuda | __main__:<cell line: 0>:17 | 18:46:25 29-03-2025\n"
          ]
        }
      ],
      "source": [
        "import tqdm\n",
        "%pip install loguru\n",
        "from loguru import logger\n",
        "import sys\n",
        "import torch\n",
        "# allow all messages\n",
        "logger.remove()\n",
        "logger_id = logger.add(sys.stderr, level=\"TRACE\", colorize=True, format=\"<level>{level}</level>: {message} | {name}:{function}:{line} | {time:HH:mm:ss DD-MM-YYYY}\")\n",
        "\n",
        "%env KAGGLEHUB_CACHE=datasets\n",
        "import kagglehub\n",
        "import os\n",
        "logger.info(\"Importing dataset to {}\", os.environ[\"KAGGLEHUB_CACHE\"])\n",
        "path = kagglehub.dataset_download(\"nikhil7280/coco-image-caption\")\n",
        "logger.info(\"Path to dataset files:\", path)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "logger.info(\"Running on {}\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c4f54e20",
      "metadata": {
        "id": "c4f54e20"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import json\n",
        "class CaptionDataset(Dataset):\n",
        "    def __init__(self, images_dir: str, json_path: str, transform=None):\n",
        "        logger.trace(\"Initializing CaptionDataset, with images_dir: {}, json_path: {}\", images_dir, json_path)\n",
        "        self.images_dir = images_dir\n",
        "        self.transform = transform\n",
        "        annotations = json.load(open(json_path, 'r'))\n",
        "        self.img_paths = {}\n",
        "        self.img_cache = {}\n",
        "        logger.trace(\"Loading annotations from {}\", json_path)\n",
        "        logger.trace(\"Loading images\")\n",
        "        for imgdata in annotations['images']:\n",
        "            id = imgdata['id']\n",
        "            path = os.path.join(self.images_dir, imgdata['file_name'])\n",
        "            self.img_paths[id] = path\n",
        "        logger.info(\"Loaded {} images\", len(self.img_paths))\n",
        "        logger.trace(\"Loading captions\")\n",
        "        self.captions = []\n",
        "        self.image_to_caption = {}\n",
        "        for imgdata in annotations['annotations']:\n",
        "            id = imgdata['image_id']\n",
        "            caption = imgdata['caption']\n",
        "            assert len(caption) > 0, \"Caption is empty\"\n",
        "            assert id in self.img_paths, \"Image ID not found in img_paths\"\n",
        "            self.captions.append((self.img_paths[id], caption))\n",
        "            inserted_index = len(self.captions) - 1\n",
        "            if id not in self.image_to_caption:\n",
        "                self.image_to_caption[id] = []\n",
        "            self.image_to_caption[id].append(inserted_index)\n",
        "        logger.trace(\"Loaded {} captions\", len(self.captions))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path, caption = self.captions[index]\n",
        "        return self.load_image(img_path), caption\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.captions)\n",
        "\n",
        "    def load_image(self, img_path):\n",
        "        logger.trace(\"Loading image from {}\", img_path)\n",
        "        if img_path in self.img_cache:\n",
        "            logger.trace(\"Image found in cache\")\n",
        "            return self.img_cache[img_path]\n",
        "        else:\n",
        "            logger.trace(\"Image not found in cache, loading from disk\")\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "            if self.transform is not None:\n",
        "                img = self.transform(img)\n",
        "            self.img_cache[img_path] = img\n",
        "            return img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2ff87064",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ff87064",
        "outputId": "d1688c8a-cbd7-4289-c8fe-f6bbda698a90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
            "  warnings.warn(\n",
            "\u001b[36m\u001b[1mTRACE\u001b[0m: Initializing CaptionDataset, with images_dir: datasets/datasets/nikhil7280/coco-image-caption/versions/1/train2014/train2014, json_path: datasets/datasets/nikhil7280/coco-image-caption/versions/1/annotations_trainval2014/annotations/captions_train2014.json | __main__:__init__:5 | 18:46:37 29-03-2025\n",
            "\u001b[36m\u001b[1mTRACE\u001b[0m: Loading annotations from datasets/datasets/nikhil7280/coco-image-caption/versions/1/annotations_trainval2014/annotations/captions_train2014.json | __main__:__init__:11 | 18:46:39 29-03-2025\n",
            "\u001b[36m\u001b[1mTRACE\u001b[0m: Loading images | __main__:__init__:12 | 18:46:39 29-03-2025\n",
            "\u001b[1mINFO\u001b[0m: Loaded 82783 images | __main__:__init__:17 | 18:46:39 29-03-2025\n",
            "\u001b[36m\u001b[1mTRACE\u001b[0m: Loading captions | __main__:__init__:18 | 18:46:39 29-03-2025\n",
            "\u001b[36m\u001b[1mTRACE\u001b[0m: Loaded 414113 captions | __main__:__init__:31 | 18:46:40 29-03-2025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datasets/datasets/nikhil7280/coco-image-caption/versions/1/train2014/train2014/COCO_train2014_000000057870.jpg\n",
            "[30990, 31048, 31071, 31129, 31282]\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "from torchvision.transforms import v2\n",
        "import random\n",
        "import os\n",
        "transform = v2.Compose([\n",
        "        v2.Resize((224, 224)),\n",
        "        #v2.RandomCrop((224, 224), pad_if_needed=True, padding_mode='symmetric'),\n",
        "        v2.ToTensor(),\n",
        "        v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "dataset = CaptionDataset(\n",
        "    images_dir=os.path.join(path, 'train2014/train2014'),\n",
        "    json_path=os.path.join(path, 'annotations_trainval2014/annotations/captions_train2014.json')\n",
        ")\n",
        "id = dataset.img_paths.keys().__iter__().__next__()\n",
        "print(dataset.img_paths.get(id))\n",
        "print(dataset.image_to_caption.get(id))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "ab4272be",
      "metadata": {
        "id": "ab4272be"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import GPT2LMHeadModel, GPT2Config, AutoTokenizer\n",
        "\n",
        "class TransformerDecoderWithCrossAttention(nn.Module):\n",
        "    def __init__(self, embedding_dim, vocab_size, max_length, gpt2_model_name='gpt2'):\n",
        "        super(TransformerDecoderWithCrossAttention, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Load GPT-2 configuration\n",
        "        gpt2_config = GPT2Config.from_pretrained(gpt2_model_name)\n",
        "        self.hidden_dim = gpt2_config.n_embd\n",
        "        self.num_heads = gpt2_config.n_head\n",
        "\n",
        "        # Linear layer to project image embeddings into GPT-2 embedding space\n",
        "        self.image_projection = nn.Linear(embedding_dim, self.hidden_dim)\n",
        "\n",
        "        # Load pre-trained GPT-2 model\n",
        "        self.gpt2 = GPT2LMHeadModel.from_pretrained(gpt2_model_name)\n",
        "\n",
        "        # Freeze GPT-2 parameters (optional, can be fine-tuned later)\n",
        "        for param in self.gpt2.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Add cross-attention layers to each GPT-2 transformer block\n",
        "        for layer in self.gpt2.transformer.h:\n",
        "            layer.cross_attention = nn.MultiheadAttention(\n",
        "                self.hidden_dim, self.num_heads, batch_first=True)\n",
        "\n",
        "        # Learnable positional embeddings for image input\n",
        "        self.image_positional_embedding = nn.Parameter(torch.randn(1, 1, self.hidden_dim))\n",
        "\n",
        "    def forward(self, image_embeddings, target_tokens=None, attention_mask=None):\n",
        "        batch_size = image_embeddings.size(0)\n",
        "\n",
        "        # Project image embeddings into GPT-2 embedding space\n",
        "        projected_image_embeddings = self.image_projection(image_embeddings) + self.image_positional_embedding\n",
        "\n",
        "        if target_tokens is not None:\n",
        "            # Prepare input tokens (prepend BOS token)\n",
        "            start_tokens = torch.ones((batch_size, 1), dtype=torch.long, device=image_embeddings.device) * self.gpt2.config.bos_token_id\n",
        "            input_tokens = torch.cat([start_tokens, target_tokens], dim=1)\n",
        "            input_embeddings = self.gpt2.transformer.wte(input_tokens)\n",
        "\n",
        "            # Create attention mask (1 for valid tokens, 0 for padding)\n",
        "            if attention_mask is None:\n",
        "                pad_token_id = self.gpt2.config.pad_token_id or 0  # Default to 0 if pad_token_id is None\n",
        "                attention_mask = (input_tokens != pad_token_id).long()\n",
        "\n",
        "            # Create causal mask to prevent future token access\n",
        "            seq_length = input_tokens.size(1)\n",
        "            causal_mask = torch.tril(torch.ones(seq_length, seq_length, device=image_embeddings.device)).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "            # Process each transformer block\n",
        "            for layer in self.gpt2.transformer.h:\n",
        "                # Self-attention\n",
        "                attn_output, _ = layer.attn(hidden_states=input_embeddings, attention_mask=causal_mask)\n",
        "\n",
        "                # Cross-attention (image embeddings as key/value)\n",
        "                cross_attn_output, _ = layer.cross_attention(attn_output, projected_image_embeddings, projected_image_embeddings)\n",
        "\n",
        "                # Residual connections\n",
        "                input_embeddings = input_embeddings + attn_output + cross_attn_output\n",
        "                input_embeddings = input_embeddings.contiguous()\n",
        "\n",
        "            # Get final logits\n",
        "            outputs = self.gpt2(inputs_embeds=input_embeddings, attention_mask=attention_mask, use_cache=False)\n",
        "            logits = outputs.logits[:, :-1, :]\n",
        "            return logits\n",
        "        else:\n",
        "            # Autoregressive inference (generation mode)\n",
        "            generated_tokens = torch.ones((batch_size, 1), dtype=torch.long, device=image_embeddings.device) * self.gpt2.config.bos_token_id\n",
        "            all_generated_tokens = generated_tokens\n",
        "            input_embeddings = self.gpt2.transformer.wte(generated_tokens)\n",
        "\n",
        "            for _ in range(self.max_length):\n",
        "                seq_length = input_embeddings.size(1)\n",
        "                causal_mask = torch.tril(torch.ones(seq_length, seq_length, device=image_embeddings.device)).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "                for layer in self.gpt2.transformer.h:\n",
        "                    attn_output, _ = layer.attn(hidden_states=input_embeddings, attention_mask=causal_mask)\n",
        "                    cross_attn_output, _ = layer.cross_attention(attn_output, projected_image_embeddings, projected_image_embeddings)\n",
        "                    input_embeddings = input_embeddings + attn_output + cross_attn_output\n",
        "                    input_embeddings = input_embeddings.contiguous()\n",
        "\n",
        "                outputs = self.gpt2(inputs_embeds=input_embeddings, use_cache=False)\n",
        "                logits = outputs.logits[:, -1, :]\n",
        "                predicted_token = torch.argmax(logits, dim=-1).unsqueeze(1)\n",
        "\n",
        "                if predicted_token.item() == self.gpt2.config.eos_token_id:\n",
        "                    break\n",
        "\n",
        "                all_generated_tokens = torch.cat([all_generated_tokens, predicted_token], dim=1)\n",
        "                input_embeddings = self.gpt2.transformer.wte(predicted_token)\n",
        "\n",
        "            return all_generated_tokens[:, 1:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "b0df6a6a",
      "metadata": {
        "id": "b0df6a6a"
      },
      "outputs": [],
      "source": [
        "from transformers import ViTModel\n",
        "from PIL import Image\n",
        "class HandmadeEncoderDecoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HandmadeEncoderDecoder, self).__init__()\n",
        "        ENCODER_CHEKCPOINT = \"google/vit-base-patch16-224-in21k\" # use the pre-trained ViT model for now\n",
        "        self.encoder = ViTModel.from_pretrained(ENCODER_CHEKCPOINT)\n",
        "        self.embedding_dim = self.encoder.config.hidden_size # the hidden size of the encoder is the size of the image embeddings generated, without any specific head, it should be a representation of the image\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")# I don't know much about the tokenizer, but we need it here for the vocab size, so the decoder knows the lenth of the *thing* which returns the probability of then next token\n",
        "        self.vocab_size = self.tokenizer.vocab_size\n",
        "        self.max_length = 20 # the maximum length of the caption, this is a hyperparameter, we can change it later\n",
        "        self.decoder = TransformerDecoderWithCrossAttention(self.embedding_dim, self.vocab_size, self.max_length)\n",
        "        logger.trace(\"HandmadeEncoderDecoder initialized with embedding_dim: {}, vocab_size: {}, max_length: {}\", self.embedding_dim, self.vocab_size, self.max_length)\n",
        "\n",
        "    def forward(self, image: Image.Image, target_tokens=None):\n",
        "        '''\n",
        "            target_tokens - used for training, represents the caption so far and because the model is autoregressive, we need to pass the previous tokens to get the next token\n",
        "            image - the image to be encoded, shape (batch_size, 3, 224, 224)\n",
        "        '''\n",
        "        batch_size = image.size(0)\n",
        "        encoder_output = self.encoder(image).last_hidden_state.mean(dim=1).unsqueeze(1)\n",
        "        #print(f'image shape: {image.shape}, encoder_output shape: {encoder_output.shape}, target_tokens shape: {target_tokens.shape if target_tokens is not None else None}')\n",
        "        decoder_output = self.decoder(encoder_output, target_tokens)\n",
        "        if target_tokens is not None:\n",
        "            return decoder_output\n",
        "        else:\n",
        "            return self.tokenizer.batch_decode(decoder_output.cpu().tolist(), skip_special_tokens=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "f2e6a9c5",
      "metadata": {
        "id": "f2e6a9c5"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Subset\n",
        "from torch.optim import Adam\n",
        "import tqdm\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "def train_model(model, train_dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for images, captions in tqdm.tqdm(train_dataloader, desc=\"Training\", leave=False):\n",
        "        images = images.to(device)\n",
        "        tokenizer = model.tokenizer\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        tokenized_captions = tokenizer(captions, return_tensors=\"pt\", padding=True, truncation=True, max_length=model.max_length).input_ids.squeeze(0)\n",
        "        target_tokens = tokenized_captions.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(images, target_tokens)\n",
        "        shift_logits = logits.contiguous().view(-1, logits.size(-1))\n",
        "        shift_labels = target_tokens.contiguous().view(-1)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        loss = criterion(shift_logits, shift_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(train_dataloader)\n",
        "\n",
        "# Example Usage:\n",
        "model = HandmadeEncoderDecoder().to(device)\n",
        "def check_gradients(module, grad_input, grad_output):\n",
        "    for grad in grad_input:\n",
        "        if torch.isnan(grad).any():\n",
        "            print(f\"NaN in gradients!\")\n",
        "model.decoder.register_backward_hook(check_gradients)\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset_train = CaptionDataset(\n",
        "    images_dir=os.path.join(path, 'train2014/train2014'),\n",
        "    json_path=os.path.join(path, 'annotations_trainval2014/annotations/captions_train2014.json'),\n",
        "    transform=transform\n",
        ")\n",
        "dataset_train = Subset(dataset_train, random.sample(range(len(dataset_train)), 32 * 10)) # use only 1000 samples for training\n",
        "dataloader = DataLoader(dataset_train, batch_size=32, shuffle=True)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = Adam(model.parameters(), lr=1e-16)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "770350e9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "770350e9",
        "outputId": "b37fae3a-116f-4c8c-9027-e6c1abf28bcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:  10%|█         | 1/10 [00:04<00:41,  4.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One train done\n",
            "Training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:  20%|██        | 2/10 [00:07<00:27,  3.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One train done\n",
            "Training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:  30%|███       | 3/10 [00:09<00:19,  2.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One train done\n",
            "Training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:  40%|████      | 4/10 [00:11<00:15,  2.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One train done\n",
            "Training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:  50%|█████     | 5/10 [00:14<00:13,  2.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One train done\n",
            "Training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:  60%|██████    | 6/10 [00:17<00:11,  2.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One train done\n",
            "Training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:  70%|███████   | 7/10 [00:21<00:09,  3.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One train done\n",
            "Training\n"
          ]
        }
      ],
      "source": [
        "logger.remove(logger_id)\n",
        "logger_id = logger.add(sys.stderr, level=\"WARNING\", colorize=True, format=\"<level>{level}</level>: {message} | {name}:{function}:{line} | {time:HH:mm:ss DD-MM-YYYY}\")\n",
        "from torch.nn.attention import SDPBackend, sdpa_kernel\n",
        "# Training loop\n",
        "num_epochs = 1\n",
        "with sdpa_kernel(SDPBackend.MATH):\n",
        "  for epoch in range(num_epochs):\n",
        "      train_loss = train_model(model, dataloader, optimizer, criterion, device)\n",
        "      print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0bd4c19",
      "metadata": {
        "id": "b0bd4c19"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    img, caption = dataset[random.randint(0, len(dataset.captions)-1)]\n",
        "    img_pixel_values = transform(img).unsqueeze(0)\n",
        "    img_pixel_values = img_pixel_values.to(device)\n",
        "    generated_caption = model(image=img_pixel_values,target_tokens=None)\n",
        "    print(generated_caption)\n",
        "img"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}