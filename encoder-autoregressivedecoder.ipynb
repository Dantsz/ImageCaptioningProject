{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f1401620",
      "metadata": {
        "id": "f1401620"
      },
      "source": [
        "# Install dependencies\n",
        "\n",
        "While I already included the dependencies in the `requirements.txt` file, there's there is another dependency that is not included there, the models themselves, which are parts of a package inside the repo containing this notebook, and because Google Colab is stupid and doens't open the whole repo, but just the notebook file, I put the models I implemented in the package and it will be installed from there using pip, specifying the git repo as install.\n",
        "\n",
        "Because I'm doing all this before really implemented any architecture besides prototype n.1, I'm installing the package in a cell and hopefully it getting reloaded automatically when I change the code in the package by pushing from my local machine to the repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fc64d144",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc64d144",
        "outputId": "ff55e585-7e04-46bb-c76a-ccfb65ad83ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://Dantsz:****@github.com/Dantsz/aiimgdetect.git\n",
            "  Cloning https://Dantsz:****@github.com/Dantsz/aiimgdetect.git to /tmp/pip-req-build-uvk91c0l\n",
            "  Running command git clone --filter=blob:none --quiet 'https://Dantsz:****@github.com/Dantsz/aiimgdetect.git' /tmp/pip-req-build-uvk91c0l\n",
            "  Resolved https://Dantsz:****@github.com/Dantsz/aiimgdetect.git to commit 7d359742bf305e2e0799c34454f6d0e0782e916d\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting loguru (from EncoderDecoderProjectComponents==0.3.96)\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Downloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: EncoderDecoderProjectComponents\n",
            "  Building wheel for EncoderDecoderProjectComponents (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for EncoderDecoderProjectComponents: filename=EncoderDecoderProjectComponents-0.3.96-py3-none-any.whl size=18279 sha256=feb8ccb5b776d817d96c3440bacab48586f9a3830445021fae90acc2e5dcc2ab\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-6du76_ph/wheels/d2/47/d9/90e20e63aeeed0f2694d951a0bdf4c78111cc1808d10537786\n",
            "Successfully built EncoderDecoderProjectComponents\n",
            "Installing collected packages: loguru, EncoderDecoderProjectComponents\n",
            "Successfully installed EncoderDecoderProjectComponents-0.3.96 loguru-0.7.3\n"
          ]
        }
      ],
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "# I often use the line magics - Stackoverflow user\n",
        "import sys\n",
        "import os\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "repo_token = None\n",
        "if IN_COLAB:\n",
        "  from google.colab import userdata\n",
        "  repo_token = userdata.get('GITHUB_TOKEN')\n",
        "repo_url = None\n",
        "if repo_token is None: #use ssh, for local development\n",
        "    repo_url = f'git+ssh://git@github.com/Dantsz/aiimgdetect.git'\n",
        "else:\n",
        "    repo_url = f'git+https://Dantsz:{repo_token}@github.com/Dantsz/aiimgdetect.git'\n",
        "\n",
        "%pip install --upgrade {repo_url}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8kWdXG537R4h",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kWdXG537R4h",
        "outputId": "fb8cfa3e-6cb6-4987-b6e5-d61acf3080af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.31.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.15)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.3\n"
          ]
        }
      ],
      "source": [
        "%pip install evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2843797",
      "metadata": {
        "id": "a2843797"
      },
      "source": [
        "# Fetch dataset\n",
        "I will be working with COCO Captions, which is a large-scale object detection, segmentation, and captioning dataset. It contains over 330k images, with more than 200k labeled images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "cf135fdd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf135fdd",
        "outputId": "2c5f251e-6102-488f-e87d-f98b779558b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: loguru in /usr/local/lib/python3.11/dist-packages (0.7.3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1mINFO\u001b[0m: Colab? : True | __main__:<cell line: 0>:10 | 10:19:29 13-05-2025\n",
            "\u001b[1mINFO\u001b[0m: Mounitng Google drive | __main__:<cell line: 0>:12 | 10:19:29 13-05-2025\n",
            "\u001b[1mINFO\u001b[0m: Python version: 3.11.12 (main, Apr  9 2025, 08:55:54) [GCC 11.4.0] | __main__:<cell line: 0>:15 | 10:19:52 13-05-2025\n",
            "\u001b[1mINFO\u001b[0m: Torch version: 2.6.0+cu124 | __main__:<cell line: 0>:16 | 10:19:52 13-05-2025\n",
            "\u001b[1mINFO\u001b[0m: Cuda available? : True | __main__:<cell line: 0>:17 | 10:19:52 13-05-2025\n",
            "\u001b[1mINFO\u001b[0m: Running on cuda | __main__:<cell line: 0>:19 | 10:19:52 13-05-2025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "env: KAGGLEHUB_CACHE=datasets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1mINFO\u001b[0m: Importing dataset to datasets | __main__:<cell line: 0>:23 | 10:19:52 13-05-2025\n",
            "\u001b[1mINFO\u001b[0m: Path to dataset files: | __main__:<cell line: 0>:25 | 10:19:57 13-05-2025\n"
          ]
        }
      ],
      "source": [
        "import tqdm\n",
        "%pip install loguru\n",
        "from loguru import logger\n",
        "import sys\n",
        "import torch\n",
        "# allow all messages\n",
        "logger.remove()\n",
        "logger_id = logger.add(sys.stderr, level=\"TRACE\", colorize=True, format=\"<level>{level}</level>: {message} | {name}:{function}:{line} | {time:HH:mm:ss DD-MM-YYYY}\")\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "logger.info(\"Colab? : {}\", IN_COLAB)\n",
        "if IN_COLAB:\n",
        "  logger.info(\"Mounitng Google drive\")\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "logger.info(\"Python version: {}\", sys.version)\n",
        "logger.info(\"Torch version: {}\", torch.__version__)\n",
        "logger.info(\"Cuda available? : {}\", torch.cuda.is_available())\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "logger.info(\"Running on {}\", device)\n",
        "%env KAGGLEHUB_CACHE=datasets\n",
        "import kagglehub\n",
        "import os\n",
        "logger.info(\"Importing dataset to {}\", os.environ[\"KAGGLEHUB_CACHE\"])\n",
        "path = kagglehub.dataset_download(\"clkmuhammed/microsoft-coco-2017-common-objects-in-context\")\n",
        "logger.info(\"Path to dataset files:\", path)\n",
        "import sys\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "__ppayBr4VZE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "__ppayBr4VZE",
        "outputId": "f701419f-721c-4c0c-faef-36cbb3065159"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-60562fe5b291>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mCOLAB_WEIGHTS_FILE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/prototype3_release13.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mLOCAL_WIEIGHTS_FILE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'prototype3_release13.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencodeco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;31m#freze the decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# how about no for once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1327\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     )\n\u001b[0;32m-> 1329\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2Model, GPT2Config\n",
        "from adic_components.prototype2 import P2GPTBlock\n",
        "from adic_components.prototype3 import P3ECDEC, P3Decoder\n",
        "from adic_components.DyT import DyT\n",
        "from adic_components.CaptionsDataset import add_bos_eos\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "gpt2_model_pretrained = GPT2Model.from_pretrained('gpt2')\n",
        "# Get model config to know vocab size and hidden size\n",
        "config = GPT2Config.from_pretrained('gpt2')\n",
        "vocab_size = config.vocab_size\n",
        "hidden_size = config.n_embd\n",
        "gpt2 = P2GPTBlock(config)\n",
        "gpt2.load_state_dict(gpt2_model_pretrained.state_dict(), strict=False)\n",
        "decoder = P3Decoder(config)\n",
        "decoder.gpt2 = gpt2\n",
        "encodeco = P3ECDEC(3, 224, 224, hidden_size, decoder)\n",
        "COLAB_WEIGHTS_FILE = '/content/drive/MyDrive/prototype3_release13.pth'\n",
        "LOCAL_WIEIGHTS_FILE = 'prototype3_release13.pth'\n",
        "model = encodeco.to(device)\n",
        "#freze the decoder\n",
        "# how about no for once\n",
        "for name, param in model.decoder.gpt2.named_parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c3e49db",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c3e49db",
        "outputId": "ec7e1a20-0df7-41f0-99e8-1a0e52f94daf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[1mINFO\u001b[0m: Loading model from file | __main__:<cell line: 0>:5 | 11:40:08 08-05-2025\n",
            "\u001b[31m\u001b[1mERROR\u001b[0m: Loading model from file failed, going with default weights | __main__:<cell line: 0>:8 | 11:40:08 08-05-2025\n"
          ]
        }
      ],
      "source": [
        "READ_WEIGHTS_FROM_FILE = True\n",
        "if READ_WEIGHTS_FROM_FILE:\n",
        "  if IN_COLAB:\n",
        "   try:\n",
        "      logger.info(\"Loading model from file\")\n",
        "      model.load_state_dict(torch.load(COLAB_WEIGHTS_FILE, map_location=torch.device(device)))\n",
        "   except:\n",
        "      logger.error(\"Loading model from file failed, going with default weights\")\n",
        "  else:\n",
        "   logger.info(\"Loading model from file\")\n",
        "   try:\n",
        "      model.load_state_dict(torch.load(LOCAL_WIEIGHTS_FILE, map_location=torch.device(device)))\n",
        "   except:\n",
        "      logger.error(\"Loading model from file failed, going with default weights\")\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "Pq9s4csxh6_i",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pq9s4csxh6_i",
        "outputId": "8994b2e0-605b-4762-f27e-3003bcf94f19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1mINFO\u001b[0m: Encode has 26509555 parameters | __main__:<cell line: 0>:5 | 10:20:30 13-05-2025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model has 307894619 total parameters of which 144857435 are trainable\n",
            "26509555\n"
          ]
        }
      ],
      "source": [
        "count_params = lambda model, trainable_only=False: sum(\n",
        "    p.numel() for p in model.parameters() if (p.requires_grad or not trainable_only)\n",
        ")\n",
        "print(f'model has {count_params(model, trainable_only=False)} total parameters of which {count_params(model, trainable_only=True)} are trainable')\n",
        "logger.info(\"Encode has {} parameters\", count_params(model.encoder))\n",
        "print(count_params(model.encoder))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "WU5yFUah5SMy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WU5yFUah5SMy",
        "outputId": "ca9647ff-a0c3-469e-ed0a-ae1a115bdafc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m\u001b[1mTRACE\u001b[0m: Initializing CaptionDataset, with images_dir: /kaggle/input/microsoft-coco-2017-common-objects-in-context/val2017, json_path: /kaggle/input/microsoft-coco-2017-common-objects-in-context/annotations_trainval2017/captions_val2017.json | adic_components.CaptionsDataset:__init__:79 | 10:20:30 13-05-2025\n",
            "\u001b[36m\u001b[1mTRACE\u001b[0m: Loading annotations from /kaggle/input/microsoft-coco-2017-common-objects-in-context/annotations_trainval2017/captions_val2017.json | adic_components.CaptionsDataset:__init__:85 | 10:20:30 13-05-2025\n",
            "\u001b[36m\u001b[1mTRACE\u001b[0m: Loading images | adic_components.CaptionsDataset:__init__:86 | 10:20:30 13-05-2025\n",
            "\u001b[1mINFO\u001b[0m: Loaded 5000 images | adic_components.CaptionsDataset:__init__:91 | 10:20:30 13-05-2025\n",
            "\u001b[36m\u001b[1mTRACE\u001b[0m: Loading captions | adic_components.CaptionsDataset:__init__:92 | 10:20:30 13-05-2025\n",
            "\u001b[36m\u001b[1mTRACE\u001b[0m: Loaded 25014 captions | adic_components.CaptionsDataset:__init__:105 | 10:20:30 13-05-2025\n",
            "\u001b[36m\u001b[1mTRACE\u001b[0m: Initializing CaptionDataset, with images_dir: /kaggle/input/microsoft-coco-2017-common-objects-in-context/val2017, json_path: /kaggle/input/microsoft-coco-2017-common-objects-in-context/annotations_trainval2017/captions_val2017.json | adic_components.CaptionsDataset:__init__:179 | 10:20:30 13-05-2025\n",
            "\u001b[36m\u001b[1mTRACE\u001b[0m: Loading annotations from /kaggle/input/microsoft-coco-2017-common-objects-in-context/annotations_trainval2017/captions_val2017.json | adic_components.CaptionsDataset:__init__:184 | 10:20:30 13-05-2025\n",
            "\u001b[36m\u001b[1mTRACE\u001b[0m: Loading images | adic_components.CaptionsDataset:__init__:185 | 10:20:30 13-05-2025\n",
            "\u001b[1mINFO\u001b[0m: Loaded 5000 images | adic_components.CaptionsDataset:__init__:190 | 10:20:30 13-05-2025\n",
            "\u001b[36m\u001b[1mTRACE\u001b[0m: Loading captions | adic_components.CaptionsDataset:__init__:191 | 10:20:30 13-05-2025\n",
            "\u001b[36m\u001b[1mTRACE\u001b[0m: Loaded 25014 captions | adic_components.CaptionsDataset:__init__:213 | 10:20:37 13-05-2025\n",
            "\u001b[36m\u001b[1mTRACE\u001b[0m: Initializing CaptionDataset, with images_dir: /kaggle/input/microsoft-coco-2017-common-objects-in-context/val2017, json_path: /kaggle/input/microsoft-coco-2017-common-objects-in-context/annotations_trainval2017/captions_val2017.json | adic_components.CaptionsDataset:__init__:179 | 10:20:37 13-05-2025\n",
            "\u001b[36m\u001b[1mTRACE\u001b[0m: Loading annotations from /kaggle/input/microsoft-coco-2017-common-objects-in-context/annotations_trainval2017/captions_val2017.json | adic_components.CaptionsDataset:__init__:184 | 10:20:37 13-05-2025\n",
            "\u001b[36m\u001b[1mTRACE\u001b[0m: Loading images | adic_components.CaptionsDataset:__init__:185 | 10:20:37 13-05-2025\n",
            "\u001b[1mINFO\u001b[0m: Loaded 5000 images | adic_components.CaptionsDataset:__init__:190 | 10:20:37 13-05-2025\n",
            "\u001b[36m\u001b[1mTRACE\u001b[0m: Loading captions | adic_components.CaptionsDataset:__init__:191 | 10:20:37 13-05-2025\n",
            "\u001b[36m\u001b[1mTRACE\u001b[0m: Loaded 25014 captions | adic_components.CaptionsDataset:__init__:213 | 10:20:44 13-05-2025\n",
            "\u001b[36m\u001b[1mTRACE\u001b[0m: Initializing CaptionDataset, with images_dir: /kaggle/input/microsoft-coco-2017-common-objects-in-context/train2017, json_path: /kaggle/input/microsoft-coco-2017-common-objects-in-context/annotations_trainval2017/captions_train2017.json | adic_components.CaptionsDataset:__init__:131 | 10:20:44 13-05-2025\n",
            "\u001b[36m\u001b[1mTRACE\u001b[0m: Loading annotations from /kaggle/input/microsoft-coco-2017-common-objects-in-context/annotations_trainval2017/captions_train2017.json | adic_components.CaptionsDataset:__init__:136 | 10:20:46 13-05-2025\n",
            "\u001b[36m\u001b[1mTRACE\u001b[0m: Loading images | adic_components.CaptionsDataset:__init__:137 | 10:20:46 13-05-2025\n",
            "\u001b[1mINFO\u001b[0m: Loaded 118287 images | adic_components.CaptionsDataset:__init__:142 | 10:20:46 13-05-2025\n",
            "\u001b[36m\u001b[1mTRACE\u001b[0m: Loading captions | adic_components.CaptionsDataset:__init__:143 | 10:20:46 13-05-2025\n",
            "\u001b[36m\u001b[1mTRACE\u001b[0m: Loaded 591753 captions | adic_components.CaptionsDataset:__init__:159 | 10:23:26 13-05-2025\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader, Subset\n",
        "from adic_components.CaptionsDataset import CaptionDataset, CaptionDatasetEager, CaptionDatasetValidation, CaptionDatasetPyCOCO, augmentation_test_transform, augmentation_train_transform\n",
        "import random\n",
        "import os\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset_sampling = CaptionDataset(\n",
        "    images_dir=os.path.join(path, 'val2017'),\n",
        "    json_path=os.path.join(path, 'annotations_trainval2017/captions_val2017.json'),\n",
        ")\n",
        "dataset_test = CaptionDatasetValidation(\n",
        "    images_dir=os.path.join(path, 'val2017'),\n",
        "    json_path=os.path.join(path, 'annotations_trainval2017/captions_val2017.json'),\n",
        "    transform=augmentation_test_transform,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "dataset_meteor = CaptionDatasetPyCOCO(\n",
        "    images_dir=os.path.join(path, 'val2017'),\n",
        "    json_path=os.path.join(path, 'annotations_trainval2017/captions_val2017.json'),\n",
        "    transform=augmentation_test_transform,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "dataset = CaptionDatasetEager(\n",
        "    images_dir=os.path.join(path, 'train2017'),\n",
        "    json_path=os.path.join(path, 'annotations_trainval2017/captions_train2017.json'),\n",
        "    transform=augmentation_train_transform,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-pDeCKcP8j8a",
      "metadata": {
        "id": "-pDeCKcP8j8a"
      },
      "source": [
        "# Setup train and test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "hRnSgrKujizD",
      "metadata": {
        "id": "hRnSgrKujizD"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from adic_components.CaptionsDataset import train_collate_fn, test_collate_fn\n",
        "import json\n",
        "\n",
        "#subset_train = len(dataset)\n",
        "#subset_test = len(dataset_test)\n",
        "#json_path = '/content/drive/MyDrive/dataset_subset.json'\n",
        "\n",
        "#if os.path.exists(json_path):\n",
        "#    with open(json_path, 'r') as f:\n",
        "#        logger.info(\"Retrieving indices\")\n",
        "#        indices = json.load(f)\n",
        "#        train_indices = indices['train_indices']\n",
        "#        test_indices = indices['test_indices']\n",
        "#else:\n",
        "#    logger.info(\"Creating indices\")\n",
        "#    train_indices = random.sample(range(len(dataset)), subset_train)\n",
        "#    test_indices = random.sample(range(len(dataset_test)), subset_test)\n",
        "#    with open(json_path, 'w') as f:\n",
        "#        json.dump({\n",
        "#            'train_indices': train_indices,\n",
        "#            'test_indices': test_indices\n",
        "#        }, f)\n",
        "\n",
        "# Create subsets\n",
        "#dataset_train = Subset(dataset, train_indices)\n",
        "#dataset_test = Subset(dataset_test, test_indices)\n",
        "dataset_train = dataset\n",
        "dataset_test = dataset_test\n",
        "batch_size = 32\n",
        "\n",
        "dataloader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=train_collate_fn, pin_memory=True, num_workers=8)\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True, collate_fn=test_collate_fn, pin_memory=True, num_workers=8)\n",
        "logger.info(\"Train dataset size: {}\", len(dataset_train))\n",
        "logger.info(\"Test dataset size: {}\", len(dataset_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "f2e6a9c5",
      "metadata": {
        "id": "f2e6a9c5"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "from torch import nn\n",
        "import tqdm\n",
        "from torch import amp\n",
        "from torch.utils.data import RandomSampler\n",
        "import contextlib\n",
        "#torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "def create_loss_mask(target_tokens: torch.Tensor) -> torch.Tensor:\n",
        "    mask = (target_tokens == tokenizer.eos_token_id) # (B,T)\n",
        "    mask = (~mask) | (mask.cumsum(dim=1) == 1)# attent to not masked tokens or the first eos/pad token, which here is padding\n",
        "    return mask\n",
        "\n",
        "def train_model(model, scheduler, train_dataloader, optimizer, criterion, device, on_test = None, on_checkpoint = None):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    pbar = tqdm.tqdm(train_dataloader, desc=\"Training\", leave=False)\n",
        "    iters = 0\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    for images, captions in pbar:\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        captions = captions.to(device, non_blocking=True)\n",
        "        input_tokens = captions[:, :-1]# Remove last token\n",
        "        target_tokens = captions[:, 1:]# Remove first token\n",
        "        optimizer.zero_grad()\n",
        "        with amp.autocast(device_type='cuda'):\n",
        "          with sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n",
        "              mask = create_loss_mask(target_tokens)\n",
        "              T = input_tokens.shape[1]\n",
        "              attn_mask = torch.triu(torch.ones(T, T), diagonal=1)\n",
        "              attn_mask = attn_mask.masked_fill(attn_mask == 1, float('-inf')).to(device)\n",
        "              logits = model(input_tokens, images, attention_mask=attn_mask)  # shape: (B, T, V)\n",
        "              logits = logits.contiguous().view(-1, logits.size(-1))# just make a big line\n",
        "              target = target_tokens.reshape(-1)# two big lines\n",
        "              loss = criterion(logits, target)# just compare the two lines\n",
        "              mask = mask.reshape(-1)\n",
        "              # Mask repeated EOS tokens\n",
        "              if mask.sum() != 0:\n",
        "                loss = loss[mask].sum() / mask.sum()\n",
        "\n",
        "              scaler.scale(loss).backward()\n",
        "              utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "              scaler.step(optimizer)\n",
        "              scaler.update()\n",
        "              iters +=1\n",
        "              if iters%2500 == 0 and on_test is not None:\n",
        "                on_checkpoint()\n",
        "              total_loss += loss.item()\n",
        "              pbar.set_postfix(loss=loss, mean=(total_loss/iters))\n",
        "              scheduler.step()\n",
        "\n",
        "\n",
        "    return total_loss / len(train_dataloader)\n",
        "\n",
        "def test_model(model, test_dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_tokens = 0\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    with torch.no_grad():\n",
        "        for images, captions, _, _ in test_dataloader:\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            captions = captions.to(device, non_blocking=True)\n",
        "            input_tokens = captions[:, :-1]\n",
        "            target_tokens = captions[:, 1:]\n",
        "\n",
        "            with torch.cuda.amp.autocast():\n",
        "                with sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n",
        "                    mask = create_loss_mask(target_tokens)\n",
        "                    logits = model(input_tokens, images)  # shape: (B, T, V)\n",
        "                    logits = logits.contiguous().view(-1, logits.size(-1))# just make a big line\n",
        "                    target = target_tokens.reshape(-1)# two big lines\n",
        "                    loss = criterion(logits, target)# just compare the two lines\n",
        "                    mask = mask.reshape(-1)\n",
        "                    if mask.sum() != 0:\n",
        "                      loss = loss[mask].sum() / mask.sum()\n",
        "                    total_loss += loss.item()\n",
        "                    total_tokens += mask.sum().item()\n",
        "\n",
        "    return total_loss / len(test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "7o17d33Y5l8u",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214,
          "referenced_widgets": [
            "d3151d608d234b58ac2a1bf1987cb861",
            "d30bf28a1bfb4d3ebf65f6081cb60ec7",
            "a9aaaeb121f74405bf1a303681887938",
            "3095d51b8b834c239b82184ae7d1af5b",
            "3fa0d52e885d40f1ba2a23201af342d8",
            "1030770761d949069dbe4efa8f071435",
            "9698ac794b6a45b8a3bc6fea8b832e04",
            "d46002591f7d4bea8aa9e8e7d6d37fe3",
            "891213c6c848422ab45c1d47fa4a60d3",
            "bcf3e79eddca46b8bee6186b098cd42c",
            "511da127193a463ea43bdbcfa40c8dc9",
            "5e4b8a6e5f1243539e9ba6c09cb4f0cc",
            "42541beb9a884d5eade9e72a040eaefd",
            "d3ed296911fb483d9ac18f17a5819e05",
            "ea6087a5f1ef4587959783b620409e0f",
            "f48f4f63f2194686866185ec3752121a",
            "280c5d5f52b047a0bed29605fa030621",
            "06f280491ab8441c963bec8943db91d5",
            "91bf776d19a244529584a6e1f53ae996",
            "07a8587d8ffc4c1c8b6e116509dfc736",
            "8e08fb20f761466ab8d158647affa510",
            "f1e9540c22e64ea19892a22822865c14",
            "1879d34204e54f51b0c736db3f0b7913",
            "0216872e24dc42e9997244b0d5df6d9f",
            "35a55851392446ca88d29c691a1ed76e",
            "860c5256d57c4cfbbb6bdde9e17876cd",
            "18c18c50e6364ce881586fef83ee80b2",
            "792ae788059844cf90a2e6f9f55a3f99",
            "33c89fc2c66a47b8b697c9d05f5a2a3e",
            "63ddeafbf7124e6a96c6edde5a11ea04",
            "b5b2cfa01b8a4df0925d4eaa802bb49c",
            "fe163bfed3264821b4056e5f59817e73",
            "8a47deb591c741be83d01277a6639f9c",
            "285da06fcdb04d439a31067602adc77d",
            "10a90e5856334773980c16a821fee10b",
            "f4b61019488844adab077e47ef1506af",
            "2310fff4c6ec4a9fac10e5071f7fba89",
            "c3b7961ab06940c7a4e7efc399812e07",
            "a00149a12e1c4f4fb29ed7d67c799088",
            "3f3b7bd1709b4eb5b4c337ae6f85c1d7",
            "9cf1dbfe484149768ab7d0fc65ed9a6c",
            "cc9c96c4bc5342f4bb0dee618341fd27",
            "f634294bda8a4721985f68b3b066963d",
            "fd40d04e4877417fb560b77423894127"
          ]
        },
        "id": "7o17d33Y5l8u",
        "outputId": "9b8d82f3-9635-49af-efe8-62e6949a2f17"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/7.02k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d3151d608d234b58ac2a1bf1987cb861"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e4b8a6e5f1243539e9ba6c09cb4f0cc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1879d34204e54f51b0c736db3f0b7913"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "285da06fcdb04d439a31067602adc77d"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import adic_components.validation_utils as vutils\n",
        "test_subset = 1000\n",
        "test_dataset = Subset(dataset_sampling, range(test_subset))\n",
        "def evaluate_model(model, subset):\n",
        "      pbar = tqdm.tqdm(subset, leave=False)\n",
        "      model.eval()\n",
        "      meteors = []\n",
        "      mean = 0\n",
        "      for img, captions in pbar:\n",
        "          with torch.no_grad():\n",
        "              img_pixel_values = augmentation_train_transform(img).unsqueeze(0).to(device)\n",
        "              generated_caption = model.generate(img_pixel_values)\n",
        "              generated_caption = tokenizer.batch_decode(generated_caption.cpu(), skip_special_tokens=True)[0]\n",
        "              meteor_score = vutils.compute_meteor_score([generated_caption], [captions])\n",
        "              meteors.append(meteor_score)\n",
        "              mean = ((len(meteors) - 1) * mean + meteor_score) / len(meteors)\n",
        "              pbar.set_postfix({\"Meteor\": meteor_score, \"Mean Meteor\": mean})\n",
        "      return mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "7be4118a",
      "metadata": {
        "id": "7be4118a"
      },
      "outputs": [],
      "source": [
        "logger.remove(logger_id)\n",
        "logger_id = logger.add(sys.stderr, level=\"WARNING\", colorize=True, format=\"<level>{level}</level>: {message} | {name}:{function}:{line} | {time:HH:mm:ss DD-MM-YYYY}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vR3Cczan6qws",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vR3Cczan6qws",
        "outputId": "5f6eb77a-e397-4f00-cbc4-3463ffbc9d92"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#drive.mount('/content/drive/',force_remount=True)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/checkpoint.pth', map_location=torch.device(device)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "770350e9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        },
        "id": "770350e9",
        "outputId": "1e755fef-bc11-4e7d-ca15-3d4a0800e391"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/18493 [00:00<?, ?it/s]<ipython-input-8-ad4ddf3d2bd9>:19: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "<ipython-input-8-ad4ddf3d2bd9>:59: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "<ipython-input-8-ad4ddf3d2bd9>:67: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25, Train Loss: 3.307148, Test Loss: 3.616295, LR:0.0004980286753286195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meteor Score: 0.2071\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/25, Train Loss: 2.724611, Test Loss: 3.415258, LR:0.0004921457902821578\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meteor Score: 0.2314\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/25, Train Loss: 2.546036, Test Loss: 3.356730, LR:0.0004824441214720629\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meteor Score: 0.2240\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/25, Train Loss: 2.447356, Test Loss: 3.269532, LR:0.0004690766700109659\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meteor Score: 0.2463\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/25, Train Loss: 2.387316, Test Loss: 3.299417, LR:0.0004522542485937369\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meteor Score: 0.2527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-9c8b3c59eccd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m   \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_training_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m   \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-ad4ddf3d2bd9>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, optimizer, criterion, device, on_test, on_checkpoint)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m               \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m               \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m               \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from torch.nn.attention import SDPBackend, sdpa_kernel\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR\n",
        "import torch.nn.utils as utils\n",
        "from adic_components.CaptionsDataset import add_bos_eos\n",
        "# Define optimizer and loss function\n",
        "optimizer = torch.optim.AdamW([\n",
        "    {'params': model.encoder.parameters(), 'lr': 5e-4, 'weight_decay': 5e-2},# original: 5e-4\n",
        "    {'params': model.decoder.parameters(), 'lr': 1e-4, 'weight_decay': 5e-2} # original: 1e-4\n",
        "])\n",
        "criterion = nn.CrossEntropyLoss(reduction='none')#ignore_index=tokenizer.eos_token_id)\n",
        "# Training loop\n",
        "\n",
        "num_epochs = 25\n",
        "steps_per_epoch = len(dataloader)\n",
        "warmup = 0.1 * steps_per_epoch * num_epochs\n",
        "print(f'''Training params:\n",
        "  num_epochs: {num_epochs}\n",
        "  steps_per_epoch: {steps_per_epoch}\n",
        "  warmup: {warmup}\n",
        "''')\n",
        "cosine = CosineAnnealingLR(optimizer, T_max=num_epochs*steps_per_epoch)\n",
        "warmup_scheduler = LinearLR(optimizer, start_factor=0.1, end_factor=1.0, total_iters=warmup)\n",
        "scheduler = SequentialLR(optimizer, schedulers=[warmup_scheduler, cosine], milestones=[warmup])\n",
        "\n",
        "\n",
        "CKECPOINTING = True\n",
        "\n",
        "def run_training_test(model):\n",
        "  score = test_model(model, dataloader_test, criterion, device)\n",
        "  return score\n",
        "\n",
        "def on_checkpoint():\n",
        "  if CKECPOINTING:\n",
        "    if IN_COLAB:\n",
        "      torch.save(model.state_dict(), '/content/drive/MyDrive/checkpoint.pth')\n",
        "    else:\n",
        "      torch.save(model.state_dict(), 'checkpoint.pth')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  train_loss = train_model(model, scheduler, dataloader, optimizer, criterion, device, on_test=run_training_test, on_checkpoint=on_checkpoint)\n",
        "  test_loss = test_model(model, dataloader_test, criterion, device)\n",
        "  lr = scheduler.get_last_lr()[0]\n",
        "  print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.6f}, Test Loss: {test_loss:.6f}, LR:{lr}')\n",
        "  meteor_score = evaluate_model(model, test_dataset)\n",
        "  print(f'Meteor Score: {meteor_score:.4f}')\n",
        "  on_checkpoint()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MoyU0vAdWiKu",
      "metadata": {
        "id": "MoyU0vAdWiKu"
      },
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "import shutil\n",
        "if IN_COLAB:\n",
        "  torch.save(model.state_dict(), COLAB_WEIGHTS_FILE)\n",
        "else:\n",
        "  torch.save(model.state_dict(), LOCAL_WIEIGHTS_FILE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "b0bd4c19",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "b0bd4c19",
        "outputId": "63fb5d8a-421e-4135-bf99-4d8510ce6921"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-f4ffd134518f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mimg_pixel_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugmentation_test_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mimg_pixel_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_pixel_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdecoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_pixel_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Original caption: {caption}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/adic_components/prototype3.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, images, max_length)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0;31m# get the last token and pass it to the decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m             \u001b[0;31m# get the last token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/adic_components/prototype3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, encoder_output, attention_mask, use_cache, past_key_values)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Encoder output shape: {}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;31m# this is a different mask than the one from custom tranformers and should be safe to not use\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mgpt2_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpt2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# the output of the GPT-2 block is (batch_size, seq_length, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpt2_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m \u001b[0;31m# the output of the GPT-2 block is (batch_size, seq_length, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m \u001b[0;31m# this is the Q projection before cross-attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/adic_components/prototype2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    296\u001b[0m                 )\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m                 outputs = block(\n\u001b[0m\u001b[1;32m    299\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m                     \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m         attn_outputs = self.attn(\n\u001b[0m\u001b[1;32m    405\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, **kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m             )\n\u001b[1;32m    334\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m             attn_output, attn_weights = attention_interface(\n\u001b[0m\u001b[1;32m    336\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m                 \u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/integrations/sdpa_attention.py\u001b[0m in \u001b[0;36msdpa_attention_forward\u001b[0;34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mis_causal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_causal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     attn_output = torch.nn.functional.scaled_dot_product_attention(\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    img, caption = dataset_sampling[random.randint(0, len(dataset_sampling.captions)-1)]\n",
        "    tokenized_captions = tokenizer(caption, padding=True, return_tensors=\"pt\", add_special_tokens=True).input_ids\n",
        "    tokenized_captions = add_bos_eos(tokenized_captions, tokenizer.bos_token_id, tokenizer.eos_token_id)\n",
        "    img_pixel_values = augmentation_test_transform(img).unsqueeze(0)\n",
        "    img_pixel_values = img_pixel_values.to(device)\n",
        "    decoder_output = model.generate(img_pixel_values, max_length=50)\n",
        "    generated = tokenizer.batch_decode(decoder_output.cpu().tolist())[0]\n",
        "    print(f'Original caption: {caption}')\n",
        "    print(f'Generated: {generated}')\n",
        "    meteor_score = vutils.compute_meteor_score([generated], [caption])\n",
        "    blue_score = vutils.compute_bleu_score([generated], [caption])\n",
        "    print(f\"METEOR:{meteor_score}, BLUE: {blue_score}\")\n",
        "img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "c248a4ab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "c248a4ab",
        "outputId": "c8862b65-c675-4ada-caea-2efabe91ae3e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-81579abb921d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0morg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mimg_pixel_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mimg_pixel_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_pixel_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mdecoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_pixel_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "url = '''\n",
        "\n",
        "https://images.pexels.com/photos/30876390/pexels-photo-30876390/free-photo-of-delicious-multi-grain-bread-with-toppings-in-artistic-still-life.jpeg?auto=compress&cs=tinysrgb&w=600&lazy=load\n",
        "\n",
        "'''\n",
        "response = requests.get(url)\n",
        "img = Image.open(BytesIO(response.content))\n",
        "org = img.copy()\n",
        "img_pixel_values = vutils.default_transform(img).unsqueeze(0)\n",
        "img_pixel_values = img_pixel_values.to(device)\n",
        "decoder_output = model.generate(img_pixel_values)\n",
        "generated = tokenizer.batch_decode(decoder_output.cpu().tolist())\n",
        "print(generated)\n",
        "org"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "siGMogW5ctSg",
      "metadata": {
        "id": "siGMogW5ctSg"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QjOyvadRl5eV",
      "metadata": {
        "id": "QjOyvadRl5eV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d3151d608d234b58ac2a1bf1987cb861": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d30bf28a1bfb4d3ebf65f6081cb60ec7",
              "IPY_MODEL_a9aaaeb121f74405bf1a303681887938",
              "IPY_MODEL_3095d51b8b834c239b82184ae7d1af5b"
            ],
            "layout": "IPY_MODEL_3fa0d52e885d40f1ba2a23201af342d8"
          }
        },
        "d30bf28a1bfb4d3ebf65f6081cb60ec7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1030770761d949069dbe4efa8f071435",
            "placeholder": "​",
            "style": "IPY_MODEL_9698ac794b6a45b8a3bc6fea8b832e04",
            "value": "Downloading builder script: 100%"
          }
        },
        "a9aaaeb121f74405bf1a303681887938": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d46002591f7d4bea8aa9e8e7d6d37fe3",
            "max": 7021,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_891213c6c848422ab45c1d47fa4a60d3",
            "value": 7021
          }
        },
        "3095d51b8b834c239b82184ae7d1af5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bcf3e79eddca46b8bee6186b098cd42c",
            "placeholder": "​",
            "style": "IPY_MODEL_511da127193a463ea43bdbcfa40c8dc9",
            "value": " 7.02k/7.02k [00:00&lt;00:00, 786kB/s]"
          }
        },
        "3fa0d52e885d40f1ba2a23201af342d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1030770761d949069dbe4efa8f071435": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9698ac794b6a45b8a3bc6fea8b832e04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d46002591f7d4bea8aa9e8e7d6d37fe3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "891213c6c848422ab45c1d47fa4a60d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bcf3e79eddca46b8bee6186b098cd42c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "511da127193a463ea43bdbcfa40c8dc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e4b8a6e5f1243539e9ba6c09cb4f0cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_42541beb9a884d5eade9e72a040eaefd",
              "IPY_MODEL_d3ed296911fb483d9ac18f17a5819e05",
              "IPY_MODEL_ea6087a5f1ef4587959783b620409e0f"
            ],
            "layout": "IPY_MODEL_f48f4f63f2194686866185ec3752121a"
          }
        },
        "42541beb9a884d5eade9e72a040eaefd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_280c5d5f52b047a0bed29605fa030621",
            "placeholder": "​",
            "style": "IPY_MODEL_06f280491ab8441c963bec8943db91d5",
            "value": "Downloading builder script: 100%"
          }
        },
        "d3ed296911fb483d9ac18f17a5819e05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91bf776d19a244529584a6e1f53ae996",
            "max": 5937,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_07a8587d8ffc4c1c8b6e116509dfc736",
            "value": 5937
          }
        },
        "ea6087a5f1ef4587959783b620409e0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e08fb20f761466ab8d158647affa510",
            "placeholder": "​",
            "style": "IPY_MODEL_f1e9540c22e64ea19892a22822865c14",
            "value": " 5.94k/5.94k [00:00&lt;00:00, 688kB/s]"
          }
        },
        "f48f4f63f2194686866185ec3752121a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "280c5d5f52b047a0bed29605fa030621": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06f280491ab8441c963bec8943db91d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91bf776d19a244529584a6e1f53ae996": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07a8587d8ffc4c1c8b6e116509dfc736": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8e08fb20f761466ab8d158647affa510": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1e9540c22e64ea19892a22822865c14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1879d34204e54f51b0c736db3f0b7913": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0216872e24dc42e9997244b0d5df6d9f",
              "IPY_MODEL_35a55851392446ca88d29c691a1ed76e",
              "IPY_MODEL_860c5256d57c4cfbbb6bdde9e17876cd"
            ],
            "layout": "IPY_MODEL_18c18c50e6364ce881586fef83ee80b2"
          }
        },
        "0216872e24dc42e9997244b0d5df6d9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_792ae788059844cf90a2e6f9f55a3f99",
            "placeholder": "​",
            "style": "IPY_MODEL_33c89fc2c66a47b8b697c9d05f5a2a3e",
            "value": "Downloading extra modules: "
          }
        },
        "35a55851392446ca88d29c691a1ed76e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63ddeafbf7124e6a96c6edde5a11ea04",
            "max": 1554,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b5b2cfa01b8a4df0925d4eaa802bb49c",
            "value": 1554
          }
        },
        "860c5256d57c4cfbbb6bdde9e17876cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe163bfed3264821b4056e5f59817e73",
            "placeholder": "​",
            "style": "IPY_MODEL_8a47deb591c741be83d01277a6639f9c",
            "value": " 4.07k/? [00:00&lt;00:00, 492kB/s]"
          }
        },
        "18c18c50e6364ce881586fef83ee80b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "792ae788059844cf90a2e6f9f55a3f99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33c89fc2c66a47b8b697c9d05f5a2a3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "63ddeafbf7124e6a96c6edde5a11ea04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5b2cfa01b8a4df0925d4eaa802bb49c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fe163bfed3264821b4056e5f59817e73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a47deb591c741be83d01277a6639f9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "285da06fcdb04d439a31067602adc77d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_10a90e5856334773980c16a821fee10b",
              "IPY_MODEL_f4b61019488844adab077e47ef1506af",
              "IPY_MODEL_2310fff4c6ec4a9fac10e5071f7fba89"
            ],
            "layout": "IPY_MODEL_c3b7961ab06940c7a4e7efc399812e07"
          }
        },
        "10a90e5856334773980c16a821fee10b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a00149a12e1c4f4fb29ed7d67c799088",
            "placeholder": "​",
            "style": "IPY_MODEL_3f3b7bd1709b4eb5b4c337ae6f85c1d7",
            "value": "Downloading extra modules: 100%"
          }
        },
        "f4b61019488844adab077e47ef1506af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cf1dbfe484149768ab7d0fc65ed9a6c",
            "max": 3344,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cc9c96c4bc5342f4bb0dee618341fd27",
            "value": 3344
          }
        },
        "2310fff4c6ec4a9fac10e5071f7fba89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f634294bda8a4721985f68b3b066963d",
            "placeholder": "​",
            "style": "IPY_MODEL_fd40d04e4877417fb560b77423894127",
            "value": " 3.34k/3.34k [00:00&lt;00:00, 413kB/s]"
          }
        },
        "c3b7961ab06940c7a4e7efc399812e07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a00149a12e1c4f4fb29ed7d67c799088": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f3b7bd1709b4eb5b4c337ae6f85c1d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9cf1dbfe484149768ab7d0fc65ed9a6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc9c96c4bc5342f4bb0dee618341fd27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f634294bda8a4721985f68b3b066963d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd40d04e4877417fb560b77423894127": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}