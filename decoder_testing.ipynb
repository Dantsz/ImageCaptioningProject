{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Output Shape: torch.Size([1, 2, 768])\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "# Load pre-trained GPT-2 with eager attention\n",
    "model = GPT2Model.from_pretrained(\"gpt2\", attn_implementation=\"eager\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Tokenize input\n",
    "text = \"Hello world\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")  # returns input_ids and attention_mask\n",
    "# input_ids shape: (B, T) → (1, 2)\n",
    "\n",
    "# STEP 1: Embed tokens (wte) directly without manually adding position_ids\n",
    "input_ids = inputs[\"input_ids\"]  # (B, T) → (1, 2)\n",
    "\n",
    "# Get token embeddings (wte) and position embeddings (wpe)\n",
    "input_embeds = model.wte(input_ids)  # (B, T, D) → (1, 2, 768)\n",
    "\n",
    "# Get hidden states with added position embeddings\n",
    "hidden_states = input_embeds  # Position embedding handling is implicit\n",
    "\n",
    "# STEP 2: Run through LayerNorm + Self-Attention from the first block\n",
    "first_block = model.h[0]\n",
    "\n",
    "# Apply layer norm before attention\n",
    "normed_hidden = first_block.ln_1(hidden_states)  # (B, T, D) → (1, 2, 768)\n",
    "\n",
    "# Run self-attention\n",
    "attn_output = first_block.attn(normed_hidden, head_mask=None, output_attentions=True)\n",
    "attn_output = attn_output[0]  # (B, T, D) → (1, 2, 768)\n",
    "# attn_output shape: (B, T, D) → (1, 2, 768)\n",
    "\n",
    "# Now you have the output just after self-attention!\n",
    "print(\"Attention Output Shape:\", attn_output.shape)  # (1, 2, 768)\n",
    "\n",
    "# Example: you can take just the last token’s output if you're doing next-token prediction\n",
    "last_token = attn_output[:, -1, :]  # (B, D) → (1, 768)\n",
    "\n",
    "# Plug this into your custom head\n",
    "custom_output = torch.nn.Linear(768, 42)(last_token)  # Example head: projecting to 42 classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['wte.weight', 'wpe.weight', 'h.0.ln_1.weight', 'h.0.ln_1.bias', 'h.0.attn.c_attn.weight', 'h.0.attn.c_attn.bias', 'h.0.attn.c_proj.weight', 'h.0.attn.c_proj.bias', 'h.0.ln_2.weight', 'h.0.ln_2.bias', 'h.0.mlp.c_fc.weight', 'h.0.mlp.c_fc.bias', 'h.0.mlp.c_proj.weight', 'h.0.mlp.c_proj.bias', 'h.1.ln_1.weight', 'h.1.ln_1.bias', 'h.1.attn.c_attn.weight', 'h.1.attn.c_attn.bias', 'h.1.attn.c_proj.weight', 'h.1.attn.c_proj.bias', 'h.1.ln_2.weight', 'h.1.ln_2.bias', 'h.1.mlp.c_fc.weight', 'h.1.mlp.c_fc.bias', 'h.1.mlp.c_proj.weight', 'h.1.mlp.c_proj.bias', 'h.2.ln_1.weight', 'h.2.ln_1.bias', 'h.2.attn.c_attn.weight', 'h.2.attn.c_attn.bias', 'h.2.attn.c_proj.weight', 'h.2.attn.c_proj.bias', 'h.2.ln_2.weight', 'h.2.ln_2.bias', 'h.2.mlp.c_fc.weight', 'h.2.mlp.c_fc.bias', 'h.2.mlp.c_proj.weight', 'h.2.mlp.c_proj.bias', 'h.3.ln_1.weight', 'h.3.ln_1.bias', 'h.3.attn.c_attn.weight', 'h.3.attn.c_attn.bias', 'h.3.attn.c_proj.weight', 'h.3.attn.c_proj.bias', 'h.3.ln_2.weight', 'h.3.ln_2.bias', 'h.3.mlp.c_fc.weight', 'h.3.mlp.c_fc.bias', 'h.3.mlp.c_proj.weight', 'h.3.mlp.c_proj.bias', 'h.4.ln_1.weight', 'h.4.ln_1.bias', 'h.4.attn.c_attn.weight', 'h.4.attn.c_attn.bias', 'h.4.attn.c_proj.weight', 'h.4.attn.c_proj.bias', 'h.4.ln_2.weight', 'h.4.ln_2.bias', 'h.4.mlp.c_fc.weight', 'h.4.mlp.c_fc.bias', 'h.4.mlp.c_proj.weight', 'h.4.mlp.c_proj.bias', 'h.5.ln_1.weight', 'h.5.ln_1.bias', 'h.5.attn.c_attn.weight', 'h.5.attn.c_attn.bias', 'h.5.attn.c_proj.weight', 'h.5.attn.c_proj.bias', 'h.5.ln_2.weight', 'h.5.ln_2.bias', 'h.5.mlp.c_fc.weight', 'h.5.mlp.c_fc.bias', 'h.5.mlp.c_proj.weight', 'h.5.mlp.c_proj.bias', 'h.6.ln_1.weight', 'h.6.ln_1.bias', 'h.6.attn.c_attn.weight', 'h.6.attn.c_attn.bias', 'h.6.attn.c_proj.weight', 'h.6.attn.c_proj.bias', 'h.6.ln_2.weight', 'h.6.ln_2.bias', 'h.6.mlp.c_fc.weight', 'h.6.mlp.c_fc.bias', 'h.6.mlp.c_proj.weight', 'h.6.mlp.c_proj.bias', 'h.7.ln_1.weight', 'h.7.ln_1.bias', 'h.7.attn.c_attn.weight', 'h.7.attn.c_attn.bias', 'h.7.attn.c_proj.weight', 'h.7.attn.c_proj.bias', 'h.7.ln_2.weight', 'h.7.ln_2.bias', 'h.7.mlp.c_fc.weight', 'h.7.mlp.c_fc.bias', 'h.7.mlp.c_proj.weight', 'h.7.mlp.c_proj.bias', 'h.8.ln_1.weight', 'h.8.ln_1.bias', 'h.8.attn.c_attn.weight', 'h.8.attn.c_attn.bias', 'h.8.attn.c_proj.weight', 'h.8.attn.c_proj.bias', 'h.8.ln_2.weight', 'h.8.ln_2.bias', 'h.8.mlp.c_fc.weight', 'h.8.mlp.c_fc.bias', 'h.8.mlp.c_proj.weight', 'h.8.mlp.c_proj.bias', 'h.9.ln_1.weight', 'h.9.ln_1.bias', 'h.9.attn.c_attn.weight', 'h.9.attn.c_attn.bias', 'h.9.attn.c_proj.weight', 'h.9.attn.c_proj.bias', 'h.9.ln_2.weight', 'h.9.ln_2.bias', 'h.9.mlp.c_fc.weight', 'h.9.mlp.c_fc.bias', 'h.9.mlp.c_proj.weight', 'h.9.mlp.c_proj.bias', 'h.10.ln_1.weight', 'h.10.ln_1.bias', 'h.10.attn.c_attn.weight', 'h.10.attn.c_attn.bias', 'h.10.attn.c_proj.weight', 'h.10.attn.c_proj.bias', 'h.10.ln_2.weight', 'h.10.ln_2.bias', 'h.10.mlp.c_fc.weight', 'h.10.mlp.c_fc.bias', 'h.10.mlp.c_proj.weight', 'h.10.mlp.c_proj.bias', 'h.11.ln_1.weight', 'h.11.ln_1.bias', 'h.11.attn.c_attn.weight', 'h.11.attn.c_attn.bias', 'h.11.attn.c_proj.weight', 'h.11.attn.c_proj.bias', 'h.11.ln_2.weight', 'h.11.ln_2.bias', 'h.11.mlp.c_fc.weight', 'h.11.mlp.c_fc.bias', 'h.11.mlp.c_proj.weight', 'h.11.mlp.c_proj.bias', 'ln_f.weight', 'ln_f.bias'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next token: ' game'\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2Config\n",
    "from adic_components.prototype2 import P2GPTBlock\n",
    "# Load GPT-2 tokenizer and base model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt2_model_pretrained = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "# Get model config to know vocab size and hidden size\n",
    "config = GPT2Config.from_pretrained('gpt2')\n",
    "vocab_size = config.vocab_size\n",
    "hidden_size = config.n_embd\n",
    "gpt2_model = P2GPTBlock(config)\n",
    "gpt2_model.load_state_dict(gpt2_model_pretrained.state_dict(), strict=False)\n",
    "gpt2_model.eval()\n",
    "\n",
    "# Define a language modeling head (linear layer that maps hidden state -> vocab)\n",
    "lm_head = nn.Linear(hidden_size, vocab_size, bias=False)\n",
    "lm_head.eval()\n",
    "\n",
    "# Tie weights if desired (like original GPT2LMHeadModel)\n",
    "lm_head.weight = gpt2_model.wte.weight  # Tie to input embedding weights\n",
    "\n",
    "# Input text\n",
    "text = \"Ive been fan of the\"\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt')  # shape: [1, seq_len]\n",
    "\n",
    "# Get hidden states from GPT2Model\n",
    "with torch.no_grad():\n",
    "    outputs = gpt2_model(input_ids)\n",
    "    last_hidden_states = outputs.last_hidden_state  # shape: [1, seq_len, hidden_size]\n",
    "\n",
    "# Get the hidden state of the last token\n",
    "last_token_hidden = last_hidden_states[:, -1, :]  # shape: [1, hidden_size]\n",
    "\n",
    "# Pass through the LM head to get logits over vocab\n",
    "logits = lm_head(last_token_hidden)  # shape: [1, vocab_size]\n",
    "\n",
    "# Predict next token\n",
    "next_token_id = torch.argmax(logits, dim=-1).item()\n",
    "next_token = tokenizer.decode([next_token_id])\n",
    "\n",
    "print(f\"Next token: '{next_token}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'GPT2Block' from 'transformers' (c:\\dev\\aiimgdetect\\.venv\\Lib\\site-packages\\transformers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[83]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GPT2Tokenizer, GPT2LMHeadModel, GPT2Block\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load pretrained model and tokenizer\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'GPT2Block' from 'transformers' (c:\\dev\\aiimgdetect\\.venv\\Lib\\site-packages\\transformers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Block\n",
    "import torch\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.eval()\n",
    "\n",
    "# Input text\n",
    "input_text = \"Ive been fan of the game for a long time and I'm sure I'll be playing it again. I'm sure I'll be playing it again.\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')  # shape: [1, seq_len]\n",
    "\n",
    "# Predict logits for next token\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    logits = outputs.logits  # shape: [1, seq_len, vocab_size]\n",
    "\n",
    "# Get logits for the last token in the sequence\n",
    "last_token_logits = logits[:, -1, :]  # shape: [1, vocab_size]\n",
    "\n",
    "# Sample or take argmax for prediction\n",
    "predicted_token_id = torch.argmax(last_token_logits, dim=-1).item()\n",
    "predicted_token = tokenizer.decode([predicted_token_id])\n",
    "\n",
    "print(f\"Next token: '{predicted_token}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
