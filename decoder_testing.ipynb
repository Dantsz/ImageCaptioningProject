{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Output Shape: torch.Size([1, 2, 768])\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "# Load pre-trained GPT-2 with eager attention\n",
    "model = GPT2Model.from_pretrained(\"gpt2\", attn_implementation=\"eager\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Tokenize input\n",
    "text = \"Hello world\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")  # returns input_ids and attention_mask\n",
    "# input_ids shape: (B, T) → (1, 2)\n",
    "\n",
    "# STEP 1: Embed tokens (wte) directly without manually adding position_ids\n",
    "input_ids = inputs[\"input_ids\"]  # (B, T) → (1, 2)\n",
    "\n",
    "# Get token embeddings (wte) and position embeddings (wpe)\n",
    "input_embeds = model.wte(input_ids)  # (B, T, D) → (1, 2, 768)\n",
    "\n",
    "# Get hidden states with added position embeddings\n",
    "hidden_states = input_embeds  # Position embedding handling is implicit\n",
    "\n",
    "# STEP 2: Run through LayerNorm + Self-Attention from the first block\n",
    "first_block = model.h[0]\n",
    "\n",
    "# Apply layer norm before attention\n",
    "normed_hidden = first_block.ln_1(hidden_states)  # (B, T, D) → (1, 2, 768)\n",
    "\n",
    "# Run self-attention\n",
    "attn_output = first_block.attn(normed_hidden, head_mask=None, output_attentions=True)\n",
    "attn_output = attn_output[0]  # (B, T, D) → (1, 2, 768)\n",
    "# attn_output shape: (B, T, D) → (1, 2, 768)\n",
    "\n",
    "# Now you have the output just after self-attention!\n",
    "print(\"Attention Output Shape:\", attn_output.shape)  # (1, 2, 768)\n",
    "\n",
    "# Example: you can take just the last token’s output if you're doing next-token prediction\n",
    "last_token = attn_output[:, -1, :]  # (B, D) → (1, 768)\n",
    "\n",
    "# Plug this into your custom head\n",
    "custom_output = torch.nn.Linear(768, 42)(last_token)  # Example head: projecting to 42 classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['wte.weight', 'wpe.weight', 'h.0.ln_1.weight', 'h.0.ln_1.bias', 'h.0.attn.c_attn.weight', 'h.0.attn.c_attn.bias', 'h.0.attn.c_proj.weight', 'h.0.attn.c_proj.bias', 'h.0.ln_2.weight', 'h.0.ln_2.bias', 'h.0.mlp.c_fc.weight', 'h.0.mlp.c_fc.bias', 'h.0.mlp.c_proj.weight', 'h.0.mlp.c_proj.bias', 'h.1.ln_1.weight', 'h.1.ln_1.bias', 'h.1.attn.c_attn.weight', 'h.1.attn.c_attn.bias', 'h.1.attn.c_proj.weight', 'h.1.attn.c_proj.bias', 'h.1.ln_2.weight', 'h.1.ln_2.bias', 'h.1.mlp.c_fc.weight', 'h.1.mlp.c_fc.bias', 'h.1.mlp.c_proj.weight', 'h.1.mlp.c_proj.bias', 'h.2.ln_1.weight', 'h.2.ln_1.bias', 'h.2.attn.c_attn.weight', 'h.2.attn.c_attn.bias', 'h.2.attn.c_proj.weight', 'h.2.attn.c_proj.bias', 'h.2.ln_2.weight', 'h.2.ln_2.bias', 'h.2.mlp.c_fc.weight', 'h.2.mlp.c_fc.bias', 'h.2.mlp.c_proj.weight', 'h.2.mlp.c_proj.bias', 'h.3.ln_1.weight', 'h.3.ln_1.bias', 'h.3.attn.c_attn.weight', 'h.3.attn.c_attn.bias', 'h.3.attn.c_proj.weight', 'h.3.attn.c_proj.bias', 'h.3.ln_2.weight', 'h.3.ln_2.bias', 'h.3.mlp.c_fc.weight', 'h.3.mlp.c_fc.bias', 'h.3.mlp.c_proj.weight', 'h.3.mlp.c_proj.bias', 'h.4.ln_1.weight', 'h.4.ln_1.bias', 'h.4.attn.c_attn.weight', 'h.4.attn.c_attn.bias', 'h.4.attn.c_proj.weight', 'h.4.attn.c_proj.bias', 'h.4.ln_2.weight', 'h.4.ln_2.bias', 'h.4.mlp.c_fc.weight', 'h.4.mlp.c_fc.bias', 'h.4.mlp.c_proj.weight', 'h.4.mlp.c_proj.bias', 'h.5.ln_1.weight', 'h.5.ln_1.bias', 'h.5.attn.c_attn.weight', 'h.5.attn.c_attn.bias', 'h.5.attn.c_proj.weight', 'h.5.attn.c_proj.bias', 'h.5.ln_2.weight', 'h.5.ln_2.bias', 'h.5.mlp.c_fc.weight', 'h.5.mlp.c_fc.bias', 'h.5.mlp.c_proj.weight', 'h.5.mlp.c_proj.bias', 'h.6.ln_1.weight', 'h.6.ln_1.bias', 'h.6.attn.c_attn.weight', 'h.6.attn.c_attn.bias', 'h.6.attn.c_proj.weight', 'h.6.attn.c_proj.bias', 'h.6.ln_2.weight', 'h.6.ln_2.bias', 'h.6.mlp.c_fc.weight', 'h.6.mlp.c_fc.bias', 'h.6.mlp.c_proj.weight', 'h.6.mlp.c_proj.bias', 'h.7.ln_1.weight', 'h.7.ln_1.bias', 'h.7.attn.c_attn.weight', 'h.7.attn.c_attn.bias', 'h.7.attn.c_proj.weight', 'h.7.attn.c_proj.bias', 'h.7.ln_2.weight', 'h.7.ln_2.bias', 'h.7.mlp.c_fc.weight', 'h.7.mlp.c_fc.bias', 'h.7.mlp.c_proj.weight', 'h.7.mlp.c_proj.bias', 'h.8.ln_1.weight', 'h.8.ln_1.bias', 'h.8.attn.c_attn.weight', 'h.8.attn.c_attn.bias', 'h.8.attn.c_proj.weight', 'h.8.attn.c_proj.bias', 'h.8.ln_2.weight', 'h.8.ln_2.bias', 'h.8.mlp.c_fc.weight', 'h.8.mlp.c_fc.bias', 'h.8.mlp.c_proj.weight', 'h.8.mlp.c_proj.bias', 'h.9.ln_1.weight', 'h.9.ln_1.bias', 'h.9.attn.c_attn.weight', 'h.9.attn.c_attn.bias', 'h.9.attn.c_proj.weight', 'h.9.attn.c_proj.bias', 'h.9.ln_2.weight', 'h.9.ln_2.bias', 'h.9.mlp.c_fc.weight', 'h.9.mlp.c_fc.bias', 'h.9.mlp.c_proj.weight', 'h.9.mlp.c_proj.bias', 'h.10.ln_1.weight', 'h.10.ln_1.bias', 'h.10.attn.c_attn.weight', 'h.10.attn.c_attn.bias', 'h.10.attn.c_proj.weight', 'h.10.attn.c_proj.bias', 'h.10.ln_2.weight', 'h.10.ln_2.bias', 'h.10.mlp.c_fc.weight', 'h.10.mlp.c_fc.bias', 'h.10.mlp.c_proj.weight', 'h.10.mlp.c_proj.bias', 'h.11.ln_1.weight', 'h.11.ln_1.bias', 'h.11.attn.c_attn.weight', 'h.11.attn.c_attn.bias', 'h.11.attn.c_proj.weight', 'h.11.attn.c_proj.bias', 'h.11.ln_2.weight', 'h.11.ln_2.bias', 'h.11.mlp.c_fc.weight', 'h.11.mlp.c_fc.bias', 'h.11.mlp.c_proj.weight', 'h.11.mlp.c_proj.bias', 'ln_f.weight', 'ln_f.bias'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 768])\n",
      "tensor([[ 4.7041e-01,  8.5578e-01, -1.3037e+00,  3.6347e-01, -7.6840e-01,\n",
      "         -8.4986e-01,  2.2346e-01,  2.6831e-02,  2.9227e-01,  4.0725e-01,\n",
      "          5.3422e-01, -2.6195e-01,  1.1958e-01, -5.9417e-01,  6.4630e-02,\n",
      "         -4.4148e-01, -1.8101e-01, -9.5986e-01,  5.5671e-01, -7.2203e-01,\n",
      "          4.3108e-02, -5.3994e-02,  9.0981e-03,  8.5935e-03, -1.1342e+00,\n",
      "         -5.3440e-01, -9.5126e-02,  7.4563e-02, -3.6980e-01, -1.3001e+00,\n",
      "          2.7258e-01, -3.4991e-01,  2.9148e-01,  2.2943e-01, -6.4044e-01,\n",
      "          9.2986e-01,  5.2259e+01, -6.4153e-01,  9.4472e-01,  2.2320e-01,\n",
      "          4.2378e-01,  6.6295e-04,  1.7511e-01, -1.3713e+00,  8.1432e-01,\n",
      "         -7.6160e-01, -5.8805e-01, -2.6967e-01,  1.2243e-01,  4.4801e-01,\n",
      "          4.5762e-02,  9.1929e-01, -4.2861e-01, -4.1535e-01,  4.3397e-01,\n",
      "         -9.1940e-02, -5.9600e-02, -5.4464e-01,  7.7408e-01, -1.2186e+00,\n",
      "          7.8279e-02,  2.8544e-01,  1.8112e-02,  1.3367e+00, -1.1104e+00,\n",
      "          3.8760e-01,  2.1491e-01, -4.4414e-02, -4.5481e-01, -1.9375e-01,\n",
      "          3.6207e-01,  6.5372e-01,  5.4069e-01,  2.2492e-01, -8.5531e-02,\n",
      "          1.5580e-01,  2.6918e-01,  2.5731e-01,  5.7786e-01,  4.0551e-02,\n",
      "         -1.5385e+00,  5.5005e-01,  1.0142e+00,  6.8474e-01, -2.0048e-01,\n",
      "         -2.9806e-01,  1.2859e+00, -1.0226e+00, -1.7235e-01,  5.6479e-02,\n",
      "         -5.2405e-01,  4.8913e-01,  1.1364e-01, -4.2945e-01,  4.0523e-02,\n",
      "          4.8131e-01,  2.2948e-01, -9.3583e-02,  2.9257e-01,  1.0015e+00,\n",
      "          5.4346e-01, -4.6814e-01,  2.2441e+00, -2.0455e-01, -2.2064e-01,\n",
      "          8.7589e-01, -4.3364e-01, -1.5284e+00, -4.2904e-02, -8.6337e-01,\n",
      "         -2.9851e-01,  6.2918e-01,  6.2201e-01,  8.3183e-01, -3.7370e-01,\n",
      "          3.0170e-01, -1.0305e+00,  5.7659e-01, -4.4423e-02,  3.4440e-01,\n",
      "          1.2968e-01,  3.3211e-01, -5.2634e-01,  4.7348e-02, -2.7247e-01,\n",
      "          6.6137e-02,  1.1333e+00,  2.1025e-01, -3.9705e-03,  3.7986e-01,\n",
      "         -1.2966e-01, -2.5630e-01, -4.5181e-01, -2.2438e-01, -5.8205e-01,\n",
      "          3.9445e-01,  3.9206e-01, -1.0777e-01,  6.0168e-01, -2.0512e-01,\n",
      "         -2.6454e-01, -1.7902e-01, -5.5462e-01, -7.5448e-02,  1.2003e-01,\n",
      "          2.1408e-01,  6.0029e-01, -2.5783e-01, -2.9246e-01, -8.5184e-01,\n",
      "         -9.3331e-02, -6.4072e-01, -4.0983e-01, -1.2973e+00, -3.9515e-02,\n",
      "          8.1969e-02,  6.9511e-01,  3.4865e-01, -3.2555e-02,  1.2942e-02,\n",
      "          2.2957e-02,  1.0987e+00, -2.3742e-01, -5.1205e-01, -6.3893e-01,\n",
      "         -6.8728e-01,  2.1315e-01, -1.3333e+00,  2.4320e-01,  4.4665e-02,\n",
      "          9.2939e-02, -2.3100e-01, -4.5706e-01,  1.0482e+00,  6.6782e-01,\n",
      "          4.9291e-01, -4.0217e-01, -4.7193e-01, -7.5011e-01, -4.9970e-02,\n",
      "         -3.4337e-01,  6.1369e-01,  1.2058e+00, -2.1496e-01,  6.3056e-02,\n",
      "          1.4553e-01,  4.5338e-01,  4.2742e-01, -5.7133e-01, -1.0104e+00,\n",
      "         -2.8932e-01, -5.4769e-01, -5.3403e-01, -1.2903e-01, -4.2357e-01,\n",
      "          1.8655e-01, -2.8073e-01, -1.5328e-01,  4.8350e-01,  4.6576e-01,\n",
      "         -8.4362e-01, -6.7518e-01, -1.4198e-01, -3.6183e-01,  8.3189e-01,\n",
      "         -1.0402e-01, -4.7795e-01, -2.3771e-01, -5.6077e-01,  3.6771e-01,\n",
      "         -1.0665e+00,  6.9398e-02, -2.6883e-01,  1.8868e-01,  4.7133e-01,\n",
      "         -7.8763e-01,  2.2380e-01, -2.7272e-01,  6.3531e-01,  5.3916e-01,\n",
      "          2.5984e-01, -4.4733e-01,  3.4282e-01, -4.3148e-01,  4.8163e-01,\n",
      "          6.9992e-01,  7.5611e-01, -3.8046e-01, -2.5768e-01, -9.0047e-01,\n",
      "         -6.3628e-02,  9.8149e-02,  4.7424e-01,  2.3433e-01, -6.5940e-02,\n",
      "         -1.0399e-01,  4.5424e-02, -1.7643e-01,  5.0646e-01, -3.4450e-01,\n",
      "         -8.6356e-01, -8.5387e-02,  1.2146e+00, -4.6196e-01,  1.3443e-01,\n",
      "         -9.7589e-03,  6.2210e-01, -1.4422e-01,  3.7450e-02,  3.2738e-02,\n",
      "          1.2355e-01,  3.4675e-01,  3.5708e-01, -9.3179e-01, -2.2448e-01,\n",
      "          2.4345e-01, -5.0128e-01,  3.2734e-01,  1.1258e+00,  1.5213e-01,\n",
      "          4.3374e-01,  1.3068e-01, -3.2059e-01, -7.2885e-01,  9.3742e-01,\n",
      "         -4.2812e-01, -1.0046e+00, -1.6332e-01, -2.0706e-01, -3.4015e-01,\n",
      "         -5.4017e-02, -2.4185e+00,  1.3914e-01,  9.1316e-01,  6.7544e-01,\n",
      "          3.4831e-01,  8.6995e-02, -5.8998e-02,  4.1848e-01,  4.3902e-01,\n",
      "         -5.2077e-01,  2.8597e-01, -2.2006e-01, -7.6510e-01,  1.7206e-01,\n",
      "         -1.4927e-01,  2.7926e-01, -4.1900e-01,  1.2711e+00, -9.7846e-02,\n",
      "         -2.4960e-01,  2.3746e-01,  2.3047e-01, -1.6079e-01,  9.2085e-01,\n",
      "          5.0803e-01,  8.8953e-02,  7.4136e-01,  4.3941e-01, -3.5865e-02,\n",
      "          3.2022e-01, -1.2932e+00,  1.0023e+00, -6.3518e-01,  5.1361e-02,\n",
      "          1.1158e-01, -2.0078e-01,  4.0536e-02, -1.0582e+00, -7.4180e-01,\n",
      "         -4.5832e-01, -2.3453e+00, -3.9275e-02, -2.5633e-01, -5.8321e+00,\n",
      "         -8.1528e-01, -9.2033e-01,  1.6114e+00,  1.4685e+00,  4.7390e-01,\n",
      "          1.0973e-01, -1.1046e+00,  5.9275e-01,  4.2238e-01, -3.3493e-01,\n",
      "          2.5080e-02,  3.1205e-01,  6.3546e-01, -6.2647e-01, -4.1956e-01,\n",
      "         -1.3931e-02,  4.5033e-01,  7.8947e-02,  3.1024e-02,  4.0546e-02,\n",
      "         -9.7107e-01, -1.0437e+00,  9.3329e-02,  5.7463e-02,  4.2156e-01,\n",
      "         -3.0437e-01,  7.3293e-01, -2.9632e-01,  4.5584e-01,  1.2554e-01,\n",
      "         -3.2186e-02,  4.2520e-01, -6.3248e-01, -8.6568e-01,  2.5322e-01,\n",
      "         -8.1297e-01, -7.9165e-02,  1.8013e-01,  5.8813e-01,  3.1948e-01,\n",
      "          1.2149e+00,  4.3302e-01,  3.1764e-01, -3.2133e-01,  3.7837e-01,\n",
      "         -6.7986e-02,  3.0366e-01, -4.5327e-01,  4.9067e-03,  6.8544e-01,\n",
      "          9.7681e-01, -5.8419e-02,  2.9753e-01, -8.8787e-01,  4.4026e-01,\n",
      "          5.4426e-01,  2.9863e-01,  4.1574e-01, -1.2446e+00, -1.5366e+01,\n",
      "         -4.0067e-01,  5.6350e-01,  1.8210e+00, -4.4267e-02, -2.4806e-01,\n",
      "          2.1775e-01,  5.6544e-01,  1.6142e-01,  1.6401e-01, -3.3533e-01,\n",
      "         -7.4734e-01,  1.2760e+00,  1.7470e-02, -6.3632e-01, -2.2536e-01,\n",
      "          9.6480e-01, -3.6369e-01,  2.7156e-01, -4.7659e-01, -9.9820e-01,\n",
      "          1.1691e+00,  3.5410e-01,  5.0578e-01,  4.8099e-01,  5.4233e-02,\n",
      "          7.1014e-01,  1.4405e-01,  1.1896e+00,  7.3487e-01,  5.0512e-01,\n",
      "         -5.5165e-01, -7.1121e-01,  2.5647e-01,  1.7586e+00, -2.4038e-01,\n",
      "         -1.0081e+00, -8.3399e-01, -1.7016e-01, -9.0653e-02, -2.6658e-01,\n",
      "         -4.4334e-02,  9.4659e-02, -6.6204e-01, -1.5211e+00,  3.7977e-02,\n",
      "         -2.9228e-01,  9.0211e-01, -1.9899e-02, -1.9606e-01,  7.2044e-02,\n",
      "         -5.9601e-02,  1.3365e-01,  2.5517e-01, -1.6999e-01,  6.0813e-02,\n",
      "          9.0037e+01,  5.5452e-01, -1.0122e+00,  2.1924e-01,  1.4097e-01,\n",
      "          3.0962e-01, -4.7301e-01,  6.3328e-02, -2.2173e-01, -7.2778e-02,\n",
      "          4.2280e-01, -4.4519e-01,  4.5088e+00, -6.0093e-01, -1.6240e-01,\n",
      "          5.3571e-01,  4.8744e-01,  1.1005e+00, -2.7963e-01, -4.0638e-02,\n",
      "          3.9265e-02, -4.5225e-01, -8.2909e-02, -9.4040e-02, -3.1893e-01,\n",
      "         -6.7253e-01, -1.2078e-01,  9.1513e-01, -4.3174e-01, -6.7316e-02,\n",
      "         -1.7507e-01, -8.4398e-02,  5.0158e-02,  3.7966e-01, -4.3693e-01,\n",
      "          1.4454e-01, -8.4842e-01,  5.8003e-01, -2.8480e-01,  1.7003e-01,\n",
      "         -4.7902e-01, -7.4043e-01,  1.2759e-01, -3.7361e-01, -3.9960e-03,\n",
      "         -1.9250e-01, -3.7674e-01, -4.1757e-01, -6.9302e-01, -1.9321e+00,\n",
      "         -8.9081e-01,  1.2080e+00, -3.1626e-01, -5.5071e-01, -2.9404e-01,\n",
      "          4.9198e-01, -1.7249e-01, -4.5246e-01, -3.4441e-02,  1.5184e-01,\n",
      "         -9.3293e-02,  3.9695e-01,  4.8962e-01,  1.1021e+00,  5.1993e-01,\n",
      "         -2.5632e-02,  1.3655e+02, -9.5203e-02,  3.1444e-01, -4.1022e-01,\n",
      "         -4.3533e-01, -1.8979e-01, -2.8959e-02, -5.3788e-01,  2.4325e-01,\n",
      "         -2.1048e-01,  8.1548e-01, -4.7535e-02, -2.4588e-01, -2.5735e-01,\n",
      "          4.9645e-01,  1.2060e+00,  1.6109e-01,  6.5150e-01, -3.2439e-01,\n",
      "         -3.5955e-01, -9.7696e-01, -9.0834e-01, -2.0782e-01, -3.6608e-01,\n",
      "          6.2493e-01,  9.2167e-01,  1.3447e+00, -1.2432e+00,  7.9346e-01,\n",
      "         -2.1700e-01, -9.3914e-01, -2.0627e+00, -1.2237e+00,  2.1955e-01,\n",
      "         -5.1847e-01,  9.8762e-01,  3.1933e-01,  2.6739e-01, -9.4110e-01,\n",
      "          4.0338e-01,  4.5656e-02, -7.8996e-01, -6.0331e-01, -4.7158e-02,\n",
      "          7.6274e-01,  3.5869e-01, -1.3479e-01, -1.7249e-01, -3.9965e-01,\n",
      "          2.9138e-01,  4.3660e-01, -3.8950e-01, -2.4214e-01,  1.4143e-01,\n",
      "         -3.7933e-02, -3.5843e-01, -2.6391e-01, -1.8405e-01,  2.3399e-01,\n",
      "         -6.7806e-01,  7.2183e-01, -5.2293e-02,  7.5551e-01, -1.1233e+00,\n",
      "         -9.3803e-01, -3.9576e-01, -1.3402e-01,  2.0753e-02,  3.8320e-01,\n",
      "          5.9294e-01,  1.0485e+00, -8.9502e-02,  1.7313e+00,  2.3739e-01,\n",
      "         -1.3879e+00,  8.2605e-01, -7.6286e-01,  4.4185e-01, -1.3197e-01,\n",
      "          7.1075e-01,  3.1283e-01, -3.7159e-01, -7.3239e-01,  1.9822e+00,\n",
      "          2.2425e-01, -3.4907e-01, -2.8074e-02,  6.4989e-01,  7.9501e-01,\n",
      "          2.5063e-01,  3.1202e-01, -1.5022e-01, -5.6962e-01,  7.4886e-03,\n",
      "         -4.4833e-01, -6.4140e-02, -2.7044e-01, -6.7690e-01, -7.1697e-01,\n",
      "         -2.0254e-01,  6.4804e-01,  4.1466e-02,  1.7197e-01, -6.9731e-01,\n",
      "         -1.2675e-02,  2.5774e-01,  3.4356e-01, -5.0939e-01,  1.7174e-01,\n",
      "         -1.1188e+00,  8.3665e-01, -5.1117e-01, -1.6626e-02,  4.1852e-01,\n",
      "          2.9356e-01,  8.0591e-01,  2.7187e-01, -5.0480e-02,  1.0755e+00,\n",
      "          8.5020e-01,  3.7682e-01, -9.2196e-01,  1.6822e-01, -4.2055e-01,\n",
      "          3.7504e-01, -8.9802e-01, -2.4902e-02,  7.4584e-01,  5.8975e-01,\n",
      "         -3.9467e-02, -8.1473e-01,  4.3829e-01,  2.1275e-01,  9.1763e-02,\n",
      "         -2.2337e-01, -2.3035e-01,  9.1675e-02,  3.4426e-01,  6.3333e-01,\n",
      "         -3.1987e+00, -3.1840e-01, -8.7859e-03,  7.7107e-01, -2.4917e-01,\n",
      "         -4.8472e-01,  1.0606e+00,  7.9237e-02, -2.6622e-01,  1.9228e-01,\n",
      "         -1.2875e+00,  1.2403e-01, -7.9603e-01, -8.0847e-01,  5.8458e-01,\n",
      "         -1.8867e-01, -2.9125e-01,  1.5478e-01, -7.9701e-01, -4.6534e-01,\n",
      "          6.0857e-01, -2.6690e-01, -3.2365e-02,  3.5367e-01,  3.5228e-01,\n",
      "         -4.1494e-01,  4.7262e-01, -7.6273e-02,  5.5087e-02, -4.8574e-01,\n",
      "         -5.6984e-02,  1.5177e-01,  4.0517e-01,  7.2228e-01, -3.8724e-01,\n",
      "         -3.4540e-01,  4.4336e-02, -3.1651e-01,  1.0394e+00, -9.4327e-02,\n",
      "          3.2671e-01,  7.1464e-01, -6.2357e-01,  8.4515e-01,  2.7850e-01,\n",
      "          4.1256e-01,  4.6793e-01, -8.6060e-01,  6.6683e-01,  7.2001e-01,\n",
      "         -1.2883e+00,  1.4458e-01, -1.6640e-01,  1.9199e-01,  1.8613e+00,\n",
      "         -2.8743e-01,  3.0180e-01,  1.0525e-01,  5.7291e-01,  2.8872e-02,\n",
      "         -1.3216e-01, -6.6860e-01, -5.5407e-01, -1.9991e-01,  6.1061e-01,\n",
      "          5.7595e-01,  1.5377e+00, -4.4229e-01, -1.5271e+00, -6.3964e-01,\n",
      "         -4.0474e-01, -5.9894e-01, -1.7076e-01, -5.0991e-01, -3.1515e-02,\n",
      "          8.4980e-01,  1.7213e-01, -3.0749e-01, -6.9064e-01, -1.0743e-01,\n",
      "          7.3308e-01,  1.6286e-01, -1.0511e-01,  4.6801e-01, -2.4147e-01,\n",
      "          5.2149e-01, -2.4340e-01,  1.5369e-01,  4.9560e-01, -1.6238e+00,\n",
      "          2.4261e-01,  9.1198e-02, -8.1619e-01, -2.9627e-01,  3.7011e-01,\n",
      "          1.7203e-01, -6.5506e-01, -7.8185e-01,  1.4080e-01, -7.5347e-01,\n",
      "          3.3460e-01,  1.0490e-01, -3.0012e-01,  1.2016e-01, -8.1676e-01,\n",
      "         -8.9024e-01, -4.6240e-02, -2.4799e-01, -6.4556e-01,  1.2312e-01,\n",
      "          2.7365e-01, -1.3381e+00,  4.1714e-01, -5.2624e-01, -3.8039e-01,\n",
      "          2.3760e-01, -4.0201e-01,  1.6863e-01, -7.7346e-01,  4.3879e-01,\n",
      "          6.8791e-01, -8.3172e-01, -5.7743e-01, -8.0608e-01,  1.7459e-02,\n",
      "         -4.5704e-01, -6.2460e-01,  1.9991e+00, -1.2149e-01, -1.4711e-01,\n",
      "         -1.0139e+00, -2.3201e-01, -6.0855e-01]])\n",
      "tensor(0.5346, grad_fn=<SelectBackward0>)\n",
      "Next token: ' of'\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2Config\n",
    "from adic_components.prototype2 import P2GPTBlock\n",
    "# Load GPT-2 tokenizer and base model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt2_model_pretrained = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "# Get model config to know vocab size and hidden size\n",
    "config = GPT2Config.from_pretrained('gpt2')\n",
    "vocab_size = config.vocab_size\n",
    "hidden_size = config.n_embd\n",
    "gpt2_model = P2GPTBlock(config)\n",
    "gpt2_model.load_state_dict(gpt2_model_pretrained.state_dict(), strict=False)\n",
    "gpt2_model.eval()\n",
    "\n",
    "# Define a language modeling head (linear layer that maps hidden state -> vocab)\n",
    "lm_head = nn.Linear(hidden_size, vocab_size, bias=False)\n",
    "lm_head.eval()\n",
    "\n",
    "# Tie weights if desired (like original GPT2LMHeadModel)\n",
    "lm_head.weight = gpt2_model.wte.weight  # Tie to input embedding weights\n",
    "\n",
    "# Input text\n",
    "text = \"Ive been fan\"\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt')  # shape: [1, seq_len]\n",
    "\n",
    "# Get hidden states from GPT2Model\n",
    "with torch.no_grad():\n",
    "    outputs = gpt2_model(input_ids)\n",
    "    last_hidden_states = outputs.last_hidden_state  # shape: [1, seq_len, hidden_size]\n",
    "    print(last_hidden_states.shape)\n",
    "\n",
    "# Get the hidden state of the last token\n",
    "last_token_hidden = last_hidden_states[:, -1, :]  # shape: [1, hidden_size]\n",
    "\n",
    "# Pass through the LM head to get logits over vocab\n",
    "logits = lm_head(last_token_hidden)  # shape: [1, vocab_size]\n",
    "#softmax\n",
    "preds = logits.softmax(dim=-1)  # shape: [1, vocab_size]\n",
    "\n",
    "\n",
    "# Predict next token\n",
    "next_token_id = torch.argmax(preds, dim=-1).item()\n",
    "print(preds[0][next_token_id])\n",
    "next_token = tokenizer.decode([next_token_id])\n",
    "\n",
    "print(f\"Next token: '{next_token}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32])\n",
      "Next token: '\n",
      "'\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.eval()\n",
    "\n",
    "# Input text\n",
    "input_text = \"Ive been fan of the game for a long time and I'm sure I'll be playing it again. I'm sure I'll be playing it again.\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')  # shape: [1, seq_len]\n",
    "\n",
    "# Predict logits for next token\n",
    "with torch.no_grad():\n",
    "    print(input_ids.shape)\n",
    "    outputs = model(input_ids)\n",
    "    logits = outputs.logits  # shape: [1, seq_len, vocab_size]\n",
    "\n",
    "# Get logits for the last token in the sequence\n",
    "last_token_logits = logits[:, -1, :]  # shape: [1, vocab_size]\n",
    "\n",
    "# Sample or take argmax for prediction\n",
    "predicted_token_id = torch.argmax(last_token_logits, dim=-1).item()\n",
    "predicted_token = tokenizer.decode([predicted_token_id])\n",
    "\n",
    "print(f\"Next token: '{predicted_token}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "from loguru import logger\n",
    "logger.remove()\n",
    "logger_id = logger.add(sys.stderr, level=\"TRACE\", colorize=True, format=\"<level>{level}</level>: {message} | {name}:{function}:{line} | {time:HH:mm:ss DD-MM-YYYY}\")\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2Config\n",
    "from adic_components.prototype2 import P2GPTBlock, P2ECDEC, P2Decoder\n",
    "# Load GPT-2 tokenizer and base model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt2_model_pretrained = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "# Get model config to know vocab size and hidden size\n",
    "config = GPT2Config.from_pretrained('gpt2')\n",
    "vocab_size = config.vocab_size\n",
    "hidden_size = config.n_embd\n",
    "gpt2_model = P2GPTBlock(config)\n",
    "gpt2_model.load_state_dict(gpt2_model_pretrained.state_dict(), strict=False)\n",
    "decoder = P2Decoder(config)\n",
    "decoder.gpt2_model = gpt2_model\n",
    "\n",
    "encodeco = P2ECDEC(3, 224, 224, hidden_size, decoder)\n",
    "\n",
    "# Define a language modeling head (linear layer that maps hidden state -> vocab)\n",
    "lm_head = nn.Linear(hidden_size, vocab_size, bias=False)\n",
    "lm_head.eval()\n",
    "\n",
    "# Tie weights if desired (like original GPT2LMHeadModel)\n",
    "lm_head.weight = gpt2_model.wte.weight  # Tie to input embedding weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mTRACE\u001b[0m: Decoder input shape: torch.Size([1, 10]) | adic_components.prototype2:forward:360 | 09:24:25 05-04-2025\n",
      "\u001b[36m\u001b[1mTRACE\u001b[0m: Encoder output shape: torch.Size([1, 196, 768]) | adic_components.prototype2:forward:361 | 09:24:25 05-04-2025\n",
      "\u001b[36m\u001b[1mTRACE\u001b[0m: Decoder output shape: torch.Size([1, 10, 768]) | adic_components.prototype2:forward:363 | 09:24:25 05-04-2025\n",
      "\u001b[36m\u001b[1mTRACE\u001b[0m: Cross attention output shape: torch.Size([1, 10, 768]) | adic_components.prototype2:forward:366 | 09:24:25 05-04-2025\n",
      "\u001b[36m\u001b[1mTRACE\u001b[0m: LM head output shape: torch.Size([1, 10, 50257]) | adic_components.prototype2:forward:368 | 09:24:26 05-04-2025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0002)\n",
      "Next token: ' Anthem'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Input text\n",
    "text = \"The SNAP Marx Encourgeois stumbling stumbling over whether rele\"\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt')# shape: [1, seq_len]\n",
    "random_image = torch.randn(1, 3, 224, 224)  # Example input tensor\n",
    "\n",
    "from adic_components.prototype2 import P2GPTBlock, P2ECDEC, P2Decoder\n",
    "# Get hidden states from GPT2Model\n",
    "with torch.autograd.set_detect_anomaly(True):\n",
    "    with torch.no_grad():\n",
    "        #outputs = gpt2_model(input_ids)\n",
    "        outputs = encodeco(input_ids, random_image)\n",
    "\n",
    "# Get the hidden state of the last token\n",
    "logits = outputs[:, -1, :]  # shape: [1, hidden_size]\n",
    "\n",
    "#softmax\n",
    "preds = logits.softmax(dim=-1)  # shape: [1, vocab_size]\n",
    "\n",
    "\n",
    "# Predict next token\n",
    "next_token_id = torch.argmax(preds, dim=-1).item()\n",
    "print(preds[0][next_token_id])\n",
    "next_token = tokenizer.decode([next_token_id])\n",
    "\n",
    "print(f\"Next token: '{next_token}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
